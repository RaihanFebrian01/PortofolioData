{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtfHpkWlIHGP9e2D3/g3Gc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaihanFebrian01/PortofolioData/blob/main/TA_Dengan_MetodeLain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1Jzii0I3YVh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52EL5k1KmUZk",
        "outputId": "b9e3f6cd-e7ab-4de9-c712-793d4525fb56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://docs.google.com/spreadsheets/d/1qJkabnTQorCao6fGv30m-EFH6C5bCECi/edit?usp=sharing&ouid=108355656034658002761&rtpof=true&sd=true'\n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "df_data = pd.read_excel(path)"
      ],
      "metadata": {
        "id": "x0QUZYLy4dL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "DaiRmuA54oqe",
        "outputId": "f197857e-fbce-4567-901a-41fde416079e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   rentangumur  sex  benjolan di kelopak mata  berair  berkabut  DM  ganjal  \\\n",
              "0            2    0                         0       0         0   0       0   \n",
              "1            3    0                         0       0         0   0       0   \n",
              "2            2    0                         0       0         0   0       0   \n",
              "3            2    0                         0       0         0   0       0   \n",
              "4            2    1                         0       0         0   0       0   \n",
              "\n",
              "   gatal  HD  HT  ...  nyeri  penglihatan kabur  pusing  sakit  sekret  silau  \\\n",
              "0      0   0   0  ...      0                  0       0      0       1      0   \n",
              "1      0   0   0  ...      1                  0       0      0       0      0   \n",
              "2      1   0   0  ...      1                  0       0      0       0      0   \n",
              "3      0   0   0  ...      0                  0       1      0       0      0   \n",
              "4      0   0   0  ...      0                  0       0      0       0      0   \n",
              "\n",
              "   terkena percikan api  terlalu lama mata melihat layar  tidak nyaman  \\\n",
              "0                     0                                0             0   \n",
              "1                     0                                0             0   \n",
              "2                     0                                0             0   \n",
              "3                     0                                0             0   \n",
              "4                     0                                0             0   \n",
              "\n",
              "         Diagnosa  \n",
              "0  konjungtivitis  \n",
              "1  konjungtivitis  \n",
              "2  konjungtivitis  \n",
              "3  konjungtivitis  \n",
              "4  konjungtivitis  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33abcc92-0b0e-47b1-9bbc-588a8e48b385\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rentangumur</th>\n",
              "      <th>sex</th>\n",
              "      <th>benjolan di kelopak mata</th>\n",
              "      <th>berair</th>\n",
              "      <th>berkabut</th>\n",
              "      <th>DM</th>\n",
              "      <th>ganjal</th>\n",
              "      <th>gatal</th>\n",
              "      <th>HD</th>\n",
              "      <th>HT</th>\n",
              "      <th>...</th>\n",
              "      <th>nyeri</th>\n",
              "      <th>penglihatan kabur</th>\n",
              "      <th>pusing</th>\n",
              "      <th>sakit</th>\n",
              "      <th>sekret</th>\n",
              "      <th>silau</th>\n",
              "      <th>terkena percikan api</th>\n",
              "      <th>terlalu lama mata melihat layar</th>\n",
              "      <th>tidak nyaman</th>\n",
              "      <th>Diagnosa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>konjungtivitis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>konjungtivitis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>konjungtivitis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>konjungtivitis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>konjungtivitis</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33abcc92-0b0e-47b1-9bbc-588a8e48b385')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-33abcc92-0b0e-47b1-9bbc-588a8e48b385 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-33abcc92-0b0e-47b1-9bbc-588a8e48b385');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f97ce34a-164a-4f09-996c-6ed1b84d7bfa\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f97ce34a-164a-4f09-996c-6ed1b84d7bfa')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f97ce34a-164a-4f09-996c-6ed1b84d7bfa button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_data.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l24yB8y4rFD",
        "outputId": "38540e9b-caaa-4103-cafe-befe45aa5647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rentangumur                        0\n",
            "sex                                0\n",
            "benjolan di kelopak mata           0\n",
            "berair                             0\n",
            "berkabut                           0\n",
            "DM                                 0\n",
            "ganjal                             0\n",
            "gatal                              0\n",
            "HD                                 0\n",
            "HT                                 0\n",
            "kacamata tidak bisa dipakai        0\n",
            "kelopak merah                      0\n",
            "kering                             0\n",
            "kontrol                            0\n",
            "kotor                              0\n",
            "mata kabur liat dekat              0\n",
            "mata kemasukan benda asing         0\n",
            "mata kemasukan gram                0\n",
            "mata merah                         0\n",
            "mata panas                         0\n",
            "melihat bayangan hitam berjalan    0\n",
            "nyeri                              0\n",
            "penglihatan kabur                  0\n",
            "pusing                             0\n",
            "sakit                              0\n",
            "sekret                             0\n",
            "silau                              0\n",
            "terkena percikan api               0\n",
            "terlalu lama mata melihat layar    0\n",
            "tidak nyaman                       0\n",
            "Diagnosa                           0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Nyoba ANN**"
      ],
      "metadata": {
        "id": "XEyKLdr3h4B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x = df_data.drop(columns = 'Diagnosa', axis = 1)\n",
        "y = df_data['Diagnosa']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=60,stratify=y, test_size=0.3)"
      ],
      "metadata": {
        "id": "X3un8gdw4ubq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scalar = StandardScaler()\n",
        "x_train_scaled = scalar.fit_transform(x_train)\n",
        "x_test_scaled = scalar.transform(x_test)"
      ],
      "metadata": {
        "id": "r8PD8cJ_JU8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_scaled.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0Y-aSppJZ95",
        "outputId": "13f25ffd-b093-4fcd-facb-5ee5f8c91c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(188, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_scaled.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no4bsBZtJxFI",
        "outputId": "770f1eaf-9b18-45bc-b074-9ef9ee022a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(63, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ],
      "metadata": {
        "id": "wxOcqvrZJ1nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZaLve0nLwdg",
        "outputId": "621e86cb-7e59-4728-b6d7-80d7a3468b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "199          corpus alineum\n",
              "110              presbiopia\n",
              "59     syndroma mata kering\n",
              "25     syndroma mata kering\n",
              "55     syndroma mata kering\n",
              "               ...         \n",
              "77               presbiopia\n",
              "51     syndroma mata kering\n",
              "12           konjungtivitis\n",
              "247          katarak imatur\n",
              "96               presbiopia\n",
              "Name: Diagnosa, Length: 188, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()"
      ],
      "metadata": {
        "id": "23_mGViALTju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_encoded = label_encoder.fit_transform(y_train)"
      ],
      "metadata": {
        "id": "A32kuE96La2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itdAS0aTL1-6",
        "outputId": "f579c76b-a69e-4917-a820-230b5f6e4c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 4, 5, 5, 5, 1, 2, 4, 1, 0, 0, 1, 5, 2, 2, 5, 1, 2, 0, 5, 5, 1,\n",
              "       0, 0, 3, 4, 4, 1, 3, 2, 2, 2, 1, 5, 1, 5, 2, 4, 1, 1, 1, 0, 1, 5,\n",
              "       5, 4, 0, 3, 5, 5, 2, 2, 2, 1, 2, 1, 1, 1, 1, 0, 1, 5, 0, 5, 3, 4,\n",
              "       4, 5, 2, 4, 0, 5, 4, 2, 4, 4, 2, 5, 1, 4, 5, 5, 2, 0, 1, 3, 4, 3,\n",
              "       4, 4, 0, 1, 3, 2, 5, 3, 0, 2, 1, 1, 2, 1, 1, 4, 5, 3, 1, 0, 4, 1,\n",
              "       2, 2, 0, 1, 4, 3, 1, 1, 4, 3, 5, 4, 3, 4, 1, 1, 4, 3, 5, 2, 4, 1,\n",
              "       5, 4, 1, 5, 2, 5, 5, 3, 1, 1, 2, 1, 3, 1, 4, 4, 1, 2, 1, 4, 5, 5,\n",
              "       1, 0, 1, 2, 1, 4, 5, 1, 5, 5, 0, 1, 1, 1, 5, 1, 5, 1, 2, 4, 5, 1,\n",
              "       4, 3, 1, 5, 1, 1, 2, 4, 5, 3, 2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model =Sequential()\n",
        "model.add(Dense(30,activation= 'relu', input_dim=30))        #input layer\n",
        "model.add(Dense(30, activation = 'relu'))                   #hidden layer\n",
        "model.add(Dense(1, activation = 'sigmoid'))                #output layer\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pd3Fv-fJ8K1",
        "outputId": "36d115de-45d4-4e74-c1b4-199e6b3d1e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 30)                930       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 30)                930       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 31        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1891 (7.39 KB)\n",
            "Trainable params: 1891 (7.39 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train_scaled, y_train_encoded, epochs=1000, batch_size=500, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqutD9RqKxKP",
        "outputId": "a4266aa0-4d3b-4cc1-8cb5-8c8212684534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: -115534.5312 - accuracy: 0.2781 - val_loss: -116529.1172 - val_accuracy: 0.3158\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -115716.1875 - accuracy: 0.2781 - val_loss: -116708.4453 - val_accuracy: 0.3158\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -115897.2578 - accuracy: 0.2781 - val_loss: -116887.5547 - val_accuracy: 0.3158\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -116077.8125 - accuracy: 0.2781 - val_loss: -117066.5000 - val_accuracy: 0.3158\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -116257.9375 - accuracy: 0.2781 - val_loss: -117245.3125 - val_accuracy: 0.3158\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -116437.6797 - accuracy: 0.2781 - val_loss: -117424.0391 - val_accuracy: 0.3158\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: -116617.1094 - accuracy: 0.2781 - val_loss: -117602.6953 - val_accuracy: 0.3158\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -116796.2969 - accuracy: 0.2781 - val_loss: -117781.3125 - val_accuracy: 0.3158\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -116975.2578 - accuracy: 0.2781 - val_loss: -117959.9375 - val_accuracy: 0.3158\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -117154.0391 - accuracy: 0.2781 - val_loss: -118138.5547 - val_accuracy: 0.3158\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -117332.6875 - accuracy: 0.2781 - val_loss: -118317.1953 - val_accuracy: 0.3158\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -117511.2188 - accuracy: 0.2781 - val_loss: -118495.9062 - val_accuracy: 0.3158\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -117689.7031 - accuracy: 0.2781 - val_loss: -118674.6562 - val_accuracy: 0.3158\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -117868.1406 - accuracy: 0.2781 - val_loss: -118853.5156 - val_accuracy: 0.3158\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -118046.5703 - accuracy: 0.2781 - val_loss: -119032.4766 - val_accuracy: 0.3158\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -118224.9688 - accuracy: 0.2781 - val_loss: -119211.5156 - val_accuracy: 0.3158\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -118403.4297 - accuracy: 0.2781 - val_loss: -119390.6562 - val_accuracy: 0.3158\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -118581.9062 - accuracy: 0.2781 - val_loss: -119569.9375 - val_accuracy: 0.3158\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -118760.4531 - accuracy: 0.2781 - val_loss: -119749.3125 - val_accuracy: 0.3158\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -118939.0781 - accuracy: 0.2781 - val_loss: -119928.8438 - val_accuracy: 0.3158\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: -119117.7734 - accuracy: 0.2781 - val_loss: -120108.5156 - val_accuracy: 0.3158\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: -119296.5781 - accuracy: 0.2781 - val_loss: -120288.3125 - val_accuracy: 0.3158\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -119475.4922 - accuracy: 0.2781 - val_loss: -120468.2656 - val_accuracy: 0.3158\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: -119654.5234 - accuracy: 0.2781 - val_loss: -120648.3828 - val_accuracy: 0.3158\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -119833.6719 - accuracy: 0.2781 - val_loss: -120828.6484 - val_accuracy: 0.3158\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -120012.9688 - accuracy: 0.2781 - val_loss: -121009.0781 - val_accuracy: 0.3158\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: -120192.4062 - accuracy: 0.2781 - val_loss: -121189.6953 - val_accuracy: 0.3158\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -120371.9844 - accuracy: 0.2781 - val_loss: -121370.4766 - val_accuracy: 0.3158\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -120551.7188 - accuracy: 0.2781 - val_loss: -121551.4375 - val_accuracy: 0.3158\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -120731.6250 - accuracy: 0.2781 - val_loss: -121732.5938 - val_accuracy: 0.3158\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -120911.7031 - accuracy: 0.2781 - val_loss: -121913.9219 - val_accuracy: 0.3158\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -121091.9375 - accuracy: 0.2781 - val_loss: -122095.4375 - val_accuracy: 0.3158\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -121272.3672 - accuracy: 0.2781 - val_loss: -122277.1172 - val_accuracy: 0.3158\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: -121452.9688 - accuracy: 0.2781 - val_loss: -122458.9844 - val_accuracy: 0.3158\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -121633.7500 - accuracy: 0.2781 - val_loss: -122641.0391 - val_accuracy: 0.3158\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -121814.6953 - accuracy: 0.2781 - val_loss: -122823.2734 - val_accuracy: 0.3158\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: -121995.8438 - accuracy: 0.2781 - val_loss: -123005.6875 - val_accuracy: 0.3158\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -122177.1719 - accuracy: 0.2781 - val_loss: -123188.3047 - val_accuracy: 0.3158\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: -122358.6719 - accuracy: 0.2781 - val_loss: -123371.1016 - val_accuracy: 0.3158\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -122540.4062 - accuracy: 0.2781 - val_loss: -123554.0781 - val_accuracy: 0.3158\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -122722.2969 - accuracy: 0.2781 - val_loss: -123737.2656 - val_accuracy: 0.3158\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -122904.3906 - accuracy: 0.2781 - val_loss: -123920.6328 - val_accuracy: 0.3158\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -123086.6875 - accuracy: 0.2781 - val_loss: -124104.2109 - val_accuracy: 0.3158\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -123269.1562 - accuracy: 0.2781 - val_loss: -124287.9766 - val_accuracy: 0.3158\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -123451.8672 - accuracy: 0.2781 - val_loss: -124471.9375 - val_accuracy: 0.3158\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -123634.7422 - accuracy: 0.2781 - val_loss: -124656.1016 - val_accuracy: 0.3158\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -123817.8203 - accuracy: 0.2781 - val_loss: -124840.4766 - val_accuracy: 0.3158\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -124001.1094 - accuracy: 0.2781 - val_loss: -125025.0547 - val_accuracy: 0.3158\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -124184.5781 - accuracy: 0.2781 - val_loss: -125209.8281 - val_accuracy: 0.3158\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -124368.2734 - accuracy: 0.2781 - val_loss: -125394.8047 - val_accuracy: 0.3158\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -124552.1641 - accuracy: 0.2781 - val_loss: -125579.9844 - val_accuracy: 0.3158\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -124736.2578 - accuracy: 0.2781 - val_loss: -125765.3516 - val_accuracy: 0.3158\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -124920.5312 - accuracy: 0.2781 - val_loss: -125950.8984 - val_accuracy: 0.3158\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -125105.0391 - accuracy: 0.2781 - val_loss: -126136.6484 - val_accuracy: 0.3158\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: -125289.7422 - accuracy: 0.2781 - val_loss: -126322.6016 - val_accuracy: 0.3158\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -125474.6641 - accuracy: 0.2781 - val_loss: -126508.7734 - val_accuracy: 0.3158\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -125659.7734 - accuracy: 0.2781 - val_loss: -126695.1328 - val_accuracy: 0.3158\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -125845.1016 - accuracy: 0.2781 - val_loss: -126881.6719 - val_accuracy: 0.3158\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -126030.6250 - accuracy: 0.2781 - val_loss: -127068.4219 - val_accuracy: 0.3158\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: -126216.3281 - accuracy: 0.2781 - val_loss: -127255.4219 - val_accuracy: 0.3158\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -126402.2734 - accuracy: 0.2781 - val_loss: -127442.5781 - val_accuracy: 0.3158\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -126588.4141 - accuracy: 0.2781 - val_loss: -127629.9766 - val_accuracy: 0.3158\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -126774.7578 - accuracy: 0.2781 - val_loss: -127817.5391 - val_accuracy: 0.3158\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -126961.3281 - accuracy: 0.2781 - val_loss: -128005.3125 - val_accuracy: 0.3158\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -127148.0703 - accuracy: 0.2781 - val_loss: -128193.3047 - val_accuracy: 0.3158\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -127335.0625 - accuracy: 0.2781 - val_loss: -128381.4844 - val_accuracy: 0.3158\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -127522.2266 - accuracy: 0.2781 - val_loss: -128569.8516 - val_accuracy: 0.3158\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -127709.6250 - accuracy: 0.2781 - val_loss: -128758.4375 - val_accuracy: 0.3158\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -127897.2344 - accuracy: 0.2781 - val_loss: -128947.2344 - val_accuracy: 0.3158\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: -128085.0156 - accuracy: 0.2781 - val_loss: -129136.2344 - val_accuracy: 0.3158\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -128273.0156 - accuracy: 0.2781 - val_loss: -129325.4453 - val_accuracy: 0.3158\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -128461.2578 - accuracy: 0.2781 - val_loss: -129514.8438 - val_accuracy: 0.3158\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -128649.6797 - accuracy: 0.2781 - val_loss: -129704.4609 - val_accuracy: 0.3158\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -128838.3203 - accuracy: 0.2781 - val_loss: -129894.2656 - val_accuracy: 0.3158\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -129027.1719 - accuracy: 0.2781 - val_loss: -130084.2734 - val_accuracy: 0.3158\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -129216.2266 - accuracy: 0.2781 - val_loss: -130274.4844 - val_accuracy: 0.3158\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -129405.4766 - accuracy: 0.2781 - val_loss: -130464.9219 - val_accuracy: 0.3158\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 94ms/step - loss: -129594.9609 - accuracy: 0.2781 - val_loss: -130655.5391 - val_accuracy: 0.3158\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -129784.6484 - accuracy: 0.2781 - val_loss: -130846.3672 - val_accuracy: 0.3158\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -129974.5469 - accuracy: 0.2781 - val_loss: -131037.4062 - val_accuracy: 0.3158\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -130164.6484 - accuracy: 0.2781 - val_loss: -131228.6875 - val_accuracy: 0.3158\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: -130354.9844 - accuracy: 0.2781 - val_loss: -131420.1250 - val_accuracy: 0.3158\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -130545.4766 - accuracy: 0.2781 - val_loss: -131611.7969 - val_accuracy: 0.3158\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -130736.2344 - accuracy: 0.2781 - val_loss: -131803.6719 - val_accuracy: 0.3158\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -130927.1719 - accuracy: 0.2781 - val_loss: -131995.7188 - val_accuracy: 0.3158\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -131118.3125 - accuracy: 0.2781 - val_loss: -132188.0000 - val_accuracy: 0.3158\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -131309.7031 - accuracy: 0.2781 - val_loss: -132380.4531 - val_accuracy: 0.3158\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -131501.2969 - accuracy: 0.2781 - val_loss: -132573.1250 - val_accuracy: 0.3158\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -131693.0625 - accuracy: 0.2781 - val_loss: -132766.0000 - val_accuracy: 0.3158\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -131885.0781 - accuracy: 0.2781 - val_loss: -132959.0781 - val_accuracy: 0.3158\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -132077.3125 - accuracy: 0.2781 - val_loss: -133152.4062 - val_accuracy: 0.3158\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -132269.7656 - accuracy: 0.2781 - val_loss: -133345.9375 - val_accuracy: 0.3158\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: -132462.4375 - accuracy: 0.2781 - val_loss: -133539.7031 - val_accuracy: 0.3158\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -132655.3281 - accuracy: 0.2781 - val_loss: -133733.6562 - val_accuracy: 0.3158\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -132848.4219 - accuracy: 0.2781 - val_loss: -133927.8438 - val_accuracy: 0.3158\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -133041.7344 - accuracy: 0.2781 - val_loss: -134122.2188 - val_accuracy: 0.3158\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -133235.2812 - accuracy: 0.2781 - val_loss: -134316.8281 - val_accuracy: 0.3158\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -133429.0312 - accuracy: 0.2781 - val_loss: -134511.6250 - val_accuracy: 0.3158\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -133623.0000 - accuracy: 0.2781 - val_loss: -134706.6719 - val_accuracy: 0.3158\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: -133817.1562 - accuracy: 0.2781 - val_loss: -134901.9062 - val_accuracy: 0.3158\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: -134011.5469 - accuracy: 0.2781 - val_loss: -135097.3750 - val_accuracy: 0.3158\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -134206.1562 - accuracy: 0.2781 - val_loss: -135293.0469 - val_accuracy: 0.3158\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -134400.9531 - accuracy: 0.2781 - val_loss: -135488.9219 - val_accuracy: 0.3158\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -134596.0000 - accuracy: 0.2781 - val_loss: -135685.0000 - val_accuracy: 0.3158\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: -134791.2500 - accuracy: 0.2781 - val_loss: -135881.2969 - val_accuracy: 0.3158\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -134986.7188 - accuracy: 0.2781 - val_loss: -136077.7969 - val_accuracy: 0.3158\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -135182.3750 - accuracy: 0.2781 - val_loss: -136274.5312 - val_accuracy: 0.3158\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -135378.2656 - accuracy: 0.2781 - val_loss: -136471.4531 - val_accuracy: 0.3158\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -135574.3594 - accuracy: 0.2781 - val_loss: -136668.6094 - val_accuracy: 0.3158\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -135770.6875 - accuracy: 0.2781 - val_loss: -136865.9531 - val_accuracy: 0.3158\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -135967.2188 - accuracy: 0.2781 - val_loss: -137063.5312 - val_accuracy: 0.3158\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -136163.9531 - accuracy: 0.2781 - val_loss: -137261.2969 - val_accuracy: 0.3158\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -136360.9219 - accuracy: 0.2781 - val_loss: -137459.2656 - val_accuracy: 0.3158\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -136558.1094 - accuracy: 0.2781 - val_loss: -137657.4531 - val_accuracy: 0.3158\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -136755.5000 - accuracy: 0.2781 - val_loss: -137855.8750 - val_accuracy: 0.3158\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -136953.1406 - accuracy: 0.2781 - val_loss: -138054.4688 - val_accuracy: 0.3158\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: -137151.0000 - accuracy: 0.2781 - val_loss: -138253.2969 - val_accuracy: 0.3158\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -137349.0781 - accuracy: 0.2781 - val_loss: -138452.3594 - val_accuracy: 0.3158\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -137547.3906 - accuracy: 0.2781 - val_loss: -138651.6094 - val_accuracy: 0.3158\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -137745.9062 - accuracy: 0.2781 - val_loss: -138851.0625 - val_accuracy: 0.3158\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -137944.6406 - accuracy: 0.2781 - val_loss: -139050.7656 - val_accuracy: 0.3158\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -138143.6094 - accuracy: 0.2781 - val_loss: -139250.6250 - val_accuracy: 0.3158\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: -138342.7969 - accuracy: 0.2781 - val_loss: -139450.7344 - val_accuracy: 0.3158\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -138542.2188 - accuracy: 0.2781 - val_loss: -139651.0469 - val_accuracy: 0.3158\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -138741.8438 - accuracy: 0.2781 - val_loss: -139851.5469 - val_accuracy: 0.3158\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -138941.7188 - accuracy: 0.2781 - val_loss: -140052.2656 - val_accuracy: 0.3158\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -139141.7969 - accuracy: 0.2781 - val_loss: -140253.2188 - val_accuracy: 0.3158\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -139342.1250 - accuracy: 0.2781 - val_loss: -140454.3750 - val_accuracy: 0.3158\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -139542.6562 - accuracy: 0.2781 - val_loss: -140655.7188 - val_accuracy: 0.3158\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -139743.3906 - accuracy: 0.2781 - val_loss: -140857.2969 - val_accuracy: 0.3158\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: -139944.3750 - accuracy: 0.2781 - val_loss: -141059.0469 - val_accuracy: 0.3158\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -140145.5469 - accuracy: 0.2781 - val_loss: -141261.0156 - val_accuracy: 0.3158\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -140346.9531 - accuracy: 0.2781 - val_loss: -141463.2031 - val_accuracy: 0.3158\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -140548.5938 - accuracy: 0.2781 - val_loss: -141665.5938 - val_accuracy: 0.3158\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -140750.4375 - accuracy: 0.2781 - val_loss: -141868.1875 - val_accuracy: 0.3158\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -140952.5156 - accuracy: 0.2781 - val_loss: -142070.9844 - val_accuracy: 0.3158\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -141154.7969 - accuracy: 0.2781 - val_loss: -142273.9844 - val_accuracy: 0.3158\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -141357.2969 - accuracy: 0.2781 - val_loss: -142477.2188 - val_accuracy: 0.3158\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: -141560.0312 - accuracy: 0.2781 - val_loss: -142680.6562 - val_accuracy: 0.3158\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -141762.9688 - accuracy: 0.2781 - val_loss: -142884.3281 - val_accuracy: 0.3158\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -141966.1250 - accuracy: 0.2781 - val_loss: -143088.2188 - val_accuracy: 0.3158\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -142169.5156 - accuracy: 0.2781 - val_loss: -143292.3125 - val_accuracy: 0.3158\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -142373.0938 - accuracy: 0.2781 - val_loss: -143496.6094 - val_accuracy: 0.3158\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -142576.9375 - accuracy: 0.2781 - val_loss: -143701.1094 - val_accuracy: 0.3158\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -142780.9688 - accuracy: 0.2781 - val_loss: -143905.8125 - val_accuracy: 0.3158\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -142985.2188 - accuracy: 0.2781 - val_loss: -144110.7344 - val_accuracy: 0.3158\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -143189.6875 - accuracy: 0.2781 - val_loss: -144315.8594 - val_accuracy: 0.3158\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -143394.3906 - accuracy: 0.2781 - val_loss: -144521.2031 - val_accuracy: 0.3158\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -143599.3125 - accuracy: 0.2781 - val_loss: -144726.7500 - val_accuracy: 0.3158\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -143804.4219 - accuracy: 0.2781 - val_loss: -144932.5000 - val_accuracy: 0.3158\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -144009.7812 - accuracy: 0.2781 - val_loss: -145138.4688 - val_accuracy: 0.3158\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: -144215.3438 - accuracy: 0.2781 - val_loss: -145344.6406 - val_accuracy: 0.3158\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -144421.1406 - accuracy: 0.2781 - val_loss: -145551.0312 - val_accuracy: 0.3158\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -144627.1406 - accuracy: 0.2781 - val_loss: -145757.6562 - val_accuracy: 0.3158\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: -144833.3594 - accuracy: 0.2781 - val_loss: -145964.5000 - val_accuracy: 0.3158\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -145039.8125 - accuracy: 0.2781 - val_loss: -146171.5469 - val_accuracy: 0.3158\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -145246.4688 - accuracy: 0.2781 - val_loss: -146378.8125 - val_accuracy: 0.3158\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: -145453.3750 - accuracy: 0.2781 - val_loss: -146586.2969 - val_accuracy: 0.3158\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: -145660.4688 - accuracy: 0.2781 - val_loss: -146793.9688 - val_accuracy: 0.3158\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: -145867.7969 - accuracy: 0.2781 - val_loss: -147001.8750 - val_accuracy: 0.3158\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -146075.3438 - accuracy: 0.2781 - val_loss: -147209.9688 - val_accuracy: 0.3158\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: -146283.1094 - accuracy: 0.2781 - val_loss: -147418.2969 - val_accuracy: 0.3158\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -146491.0938 - accuracy: 0.2781 - val_loss: -147626.8125 - val_accuracy: 0.3158\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -146699.2969 - accuracy: 0.2781 - val_loss: -147835.6250 - val_accuracy: 0.3158\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -146907.7188 - accuracy: 0.2781 - val_loss: -148044.6250 - val_accuracy: 0.3158\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -147116.3594 - accuracy: 0.2781 - val_loss: -148253.8281 - val_accuracy: 0.3158\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -147325.2344 - accuracy: 0.2781 - val_loss: -148463.2969 - val_accuracy: 0.3158\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -147534.3125 - accuracy: 0.2781 - val_loss: -148672.9375 - val_accuracy: 0.3158\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -147743.6250 - accuracy: 0.2781 - val_loss: -148882.8281 - val_accuracy: 0.3158\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -147953.1562 - accuracy: 0.2781 - val_loss: -149092.9219 - val_accuracy: 0.3158\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -148162.9062 - accuracy: 0.2781 - val_loss: -149303.2031 - val_accuracy: 0.3158\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -148372.8750 - accuracy: 0.2781 - val_loss: -149513.7188 - val_accuracy: 0.3158\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -148583.0625 - accuracy: 0.2781 - val_loss: -149724.4531 - val_accuracy: 0.3158\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -148793.4844 - accuracy: 0.2781 - val_loss: -149935.3750 - val_accuracy: 0.3158\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -149004.1250 - accuracy: 0.2781 - val_loss: -150146.5781 - val_accuracy: 0.3158\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -149214.9688 - accuracy: 0.2781 - val_loss: -150357.9531 - val_accuracy: 0.3158\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -149426.0312 - accuracy: 0.2781 - val_loss: -150569.5781 - val_accuracy: 0.3158\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -149637.3438 - accuracy: 0.2781 - val_loss: -150781.3906 - val_accuracy: 0.3158\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -149848.8750 - accuracy: 0.2781 - val_loss: -150993.4219 - val_accuracy: 0.3158\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -150060.6094 - accuracy: 0.2781 - val_loss: -151205.6562 - val_accuracy: 0.3158\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -150272.5781 - accuracy: 0.2781 - val_loss: -151418.1094 - val_accuracy: 0.3158\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: -150484.7656 - accuracy: 0.2781 - val_loss: -151630.7500 - val_accuracy: 0.3158\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -150697.1719 - accuracy: 0.2781 - val_loss: -151843.6406 - val_accuracy: 0.3158\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -150909.7969 - accuracy: 0.2781 - val_loss: -152056.7344 - val_accuracy: 0.3158\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -151122.6562 - accuracy: 0.2781 - val_loss: -152270.0312 - val_accuracy: 0.3158\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -151335.7344 - accuracy: 0.2781 - val_loss: -152483.5312 - val_accuracy: 0.3158\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -151549.0469 - accuracy: 0.2781 - val_loss: -152697.2656 - val_accuracy: 0.3158\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: -151762.5469 - accuracy: 0.2781 - val_loss: -152911.2188 - val_accuracy: 0.3158\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -151976.2969 - accuracy: 0.2781 - val_loss: -153125.3906 - val_accuracy: 0.3158\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -152190.2656 - accuracy: 0.2781 - val_loss: -153339.8125 - val_accuracy: 0.3158\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -152404.4531 - accuracy: 0.2781 - val_loss: -153554.4375 - val_accuracy: 0.3158\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -152618.8594 - accuracy: 0.2781 - val_loss: -153769.2656 - val_accuracy: 0.3158\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -152833.4844 - accuracy: 0.2781 - val_loss: -153984.2969 - val_accuracy: 0.3158\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -153048.3594 - accuracy: 0.2781 - val_loss: -154199.5625 - val_accuracy: 0.3158\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -153263.4375 - accuracy: 0.2781 - val_loss: -154415.0312 - val_accuracy: 0.3158\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -153478.7500 - accuracy: 0.2781 - val_loss: -154630.7031 - val_accuracy: 0.3158\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -153694.2812 - accuracy: 0.2781 - val_loss: -154846.6250 - val_accuracy: 0.3158\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: -153910.0312 - accuracy: 0.2781 - val_loss: -155062.7188 - val_accuracy: 0.3158\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -154126.0156 - accuracy: 0.2781 - val_loss: -155279.0781 - val_accuracy: 0.3158\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -154342.2188 - accuracy: 0.2781 - val_loss: -155495.6562 - val_accuracy: 0.3158\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -154558.6406 - accuracy: 0.2781 - val_loss: -155712.4531 - val_accuracy: 0.3158\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -154775.2969 - accuracy: 0.2781 - val_loss: -155929.4531 - val_accuracy: 0.3158\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -154992.1719 - accuracy: 0.2781 - val_loss: -156146.6719 - val_accuracy: 0.3158\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -155209.2500 - accuracy: 0.2781 - val_loss: -156364.1094 - val_accuracy: 0.3158\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -155426.5781 - accuracy: 0.2781 - val_loss: -156581.7656 - val_accuracy: 0.3158\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -155644.1406 - accuracy: 0.2781 - val_loss: -156799.6250 - val_accuracy: 0.3158\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -155861.9062 - accuracy: 0.2781 - val_loss: -157017.6875 - val_accuracy: 0.3158\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -156079.9062 - accuracy: 0.2781 - val_loss: -157235.9844 - val_accuracy: 0.3158\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -156298.1250 - accuracy: 0.2781 - val_loss: -157454.4844 - val_accuracy: 0.3158\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -156516.5781 - accuracy: 0.2781 - val_loss: -157673.2656 - val_accuracy: 0.3158\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -156735.2500 - accuracy: 0.2781 - val_loss: -157892.2031 - val_accuracy: 0.3158\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -156954.1562 - accuracy: 0.2781 - val_loss: -158111.3906 - val_accuracy: 0.3158\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -157173.2812 - accuracy: 0.2781 - val_loss: -158330.7969 - val_accuracy: 0.3158\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -157392.6250 - accuracy: 0.2781 - val_loss: -158550.4062 - val_accuracy: 0.3158\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: -157612.1875 - accuracy: 0.2781 - val_loss: -158770.2344 - val_accuracy: 0.3158\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -157832.0000 - accuracy: 0.2781 - val_loss: -158990.2969 - val_accuracy: 0.3158\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -158052.0312 - accuracy: 0.2781 - val_loss: -159210.5469 - val_accuracy: 0.3158\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -158272.2656 - accuracy: 0.2781 - val_loss: -159431.0469 - val_accuracy: 0.3158\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -158492.7500 - accuracy: 0.2781 - val_loss: -159651.7500 - val_accuracy: 0.3158\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -158713.4688 - accuracy: 0.2781 - val_loss: -159872.6719 - val_accuracy: 0.3158\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -158934.3906 - accuracy: 0.2781 - val_loss: -160093.8281 - val_accuracy: 0.3158\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: -159155.5469 - accuracy: 0.2781 - val_loss: -160315.1875 - val_accuracy: 0.3158\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -159376.9531 - accuracy: 0.2781 - val_loss: -160536.7969 - val_accuracy: 0.3158\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -159598.5625 - accuracy: 0.2781 - val_loss: -160758.6250 - val_accuracy: 0.3158\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -159820.4062 - accuracy: 0.2781 - val_loss: -160980.6406 - val_accuracy: 0.3158\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -160042.4531 - accuracy: 0.2781 - val_loss: -161202.9062 - val_accuracy: 0.3158\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -160264.7656 - accuracy: 0.2781 - val_loss: -161425.3750 - val_accuracy: 0.3158\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: -160487.2969 - accuracy: 0.2781 - val_loss: -161648.0469 - val_accuracy: 0.3158\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -160710.0312 - accuracy: 0.2781 - val_loss: -161870.9375 - val_accuracy: 0.3158\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -160933.0156 - accuracy: 0.2781 - val_loss: -162094.0625 - val_accuracy: 0.3158\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -161156.2188 - accuracy: 0.2781 - val_loss: -162317.3906 - val_accuracy: 0.3158\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -161379.6406 - accuracy: 0.2781 - val_loss: -162540.9844 - val_accuracy: 0.3158\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -161603.2969 - accuracy: 0.2781 - val_loss: -162764.7969 - val_accuracy: 0.3158\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -161827.1875 - accuracy: 0.2781 - val_loss: -162988.7969 - val_accuracy: 0.3158\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -162051.3125 - accuracy: 0.2781 - val_loss: -163213.0469 - val_accuracy: 0.3158\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -162275.6562 - accuracy: 0.2781 - val_loss: -163437.5156 - val_accuracy: 0.3158\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: -162500.2188 - accuracy: 0.2781 - val_loss: -163662.1875 - val_accuracy: 0.3158\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -162725.0312 - accuracy: 0.2781 - val_loss: -163887.0938 - val_accuracy: 0.3158\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -162950.0625 - accuracy: 0.2781 - val_loss: -164112.1875 - val_accuracy: 0.3158\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -163175.3438 - accuracy: 0.2781 - val_loss: -164337.5312 - val_accuracy: 0.3158\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -163400.8125 - accuracy: 0.2781 - val_loss: -164563.0469 - val_accuracy: 0.3158\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -163626.5469 - accuracy: 0.2781 - val_loss: -164788.8750 - val_accuracy: 0.3158\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -163852.4844 - accuracy: 0.2781 - val_loss: -165014.8906 - val_accuracy: 0.3158\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -164078.6562 - accuracy: 0.2781 - val_loss: -165241.1250 - val_accuracy: 0.3158\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: -164305.0625 - accuracy: 0.2781 - val_loss: -165467.5938 - val_accuracy: 0.3158\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -164531.6875 - accuracy: 0.2781 - val_loss: -165694.2656 - val_accuracy: 0.3158\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -164758.5625 - accuracy: 0.2781 - val_loss: -165921.1562 - val_accuracy: 0.3158\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -164985.6406 - accuracy: 0.2781 - val_loss: -166148.2969 - val_accuracy: 0.3158\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -165212.9688 - accuracy: 0.2781 - val_loss: -166375.6250 - val_accuracy: 0.3158\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -165440.5156 - accuracy: 0.2781 - val_loss: -166603.2031 - val_accuracy: 0.3158\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: -165668.3125 - accuracy: 0.2781 - val_loss: -166830.9844 - val_accuracy: 0.3158\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -165896.3125 - accuracy: 0.2781 - val_loss: -167059.0156 - val_accuracy: 0.3158\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -166124.5469 - accuracy: 0.2781 - val_loss: -167287.2656 - val_accuracy: 0.3158\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: -166353.0312 - accuracy: 0.2781 - val_loss: -167515.7969 - val_accuracy: 0.3158\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: -166581.7344 - accuracy: 0.2781 - val_loss: -167744.5000 - val_accuracy: 0.3158\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -166810.6719 - accuracy: 0.2781 - val_loss: -167973.4219 - val_accuracy: 0.3158\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -167039.8438 - accuracy: 0.2781 - val_loss: -168202.5625 - val_accuracy: 0.3158\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -167269.2656 - accuracy: 0.2781 - val_loss: -168431.9531 - val_accuracy: 0.3158\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -167498.9062 - accuracy: 0.2781 - val_loss: -168661.5469 - val_accuracy: 0.3158\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -167728.7969 - accuracy: 0.2781 - val_loss: -168891.3750 - val_accuracy: 0.3158\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: -167958.9062 - accuracy: 0.2781 - val_loss: -169121.4062 - val_accuracy: 0.3158\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: -168189.2500 - accuracy: 0.2781 - val_loss: -169351.6562 - val_accuracy: 0.3158\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -168419.8281 - accuracy: 0.2781 - val_loss: -169582.1719 - val_accuracy: 0.3158\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -168650.6562 - accuracy: 0.2781 - val_loss: -169812.9375 - val_accuracy: 0.3158\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -168881.7031 - accuracy: 0.2781 - val_loss: -170043.9531 - val_accuracy: 0.3158\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -169112.9531 - accuracy: 0.2781 - val_loss: -170275.1250 - val_accuracy: 0.3158\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -169344.4844 - accuracy: 0.2781 - val_loss: -170506.5781 - val_accuracy: 0.3158\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -169576.2188 - accuracy: 0.2781 - val_loss: -170738.2031 - val_accuracy: 0.3158\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -169808.2031 - accuracy: 0.2781 - val_loss: -170970.0781 - val_accuracy: 0.3158\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -170040.4062 - accuracy: 0.2781 - val_loss: -171202.1406 - val_accuracy: 0.3158\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -170272.8594 - accuracy: 0.2781 - val_loss: -171434.4688 - val_accuracy: 0.3158\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -170505.5156 - accuracy: 0.2781 - val_loss: -171666.9844 - val_accuracy: 0.3158\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -170738.4219 - accuracy: 0.2781 - val_loss: -171899.7188 - val_accuracy: 0.3158\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -170971.5469 - accuracy: 0.2781 - val_loss: -172132.7031 - val_accuracy: 0.3158\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -171204.9062 - accuracy: 0.2781 - val_loss: -172365.8750 - val_accuracy: 0.3158\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -171438.5156 - accuracy: 0.2781 - val_loss: -172599.2969 - val_accuracy: 0.3158\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -171672.3594 - accuracy: 0.2781 - val_loss: -172832.9062 - val_accuracy: 0.3158\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -171906.4062 - accuracy: 0.2781 - val_loss: -173066.7812 - val_accuracy: 0.3158\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -172140.7031 - accuracy: 0.2781 - val_loss: -173300.9219 - val_accuracy: 0.3158\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -172375.2500 - accuracy: 0.2781 - val_loss: -173535.2812 - val_accuracy: 0.3158\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: -172610.0000 - accuracy: 0.2781 - val_loss: -173769.8594 - val_accuracy: 0.3158\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -172845.0000 - accuracy: 0.2781 - val_loss: -174004.6562 - val_accuracy: 0.3158\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -173080.2500 - accuracy: 0.2781 - val_loss: -174239.7031 - val_accuracy: 0.3158\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -173315.7188 - accuracy: 0.2781 - val_loss: -174474.9219 - val_accuracy: 0.3158\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -173551.3906 - accuracy: 0.2781 - val_loss: -174710.3906 - val_accuracy: 0.3158\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: -173787.3438 - accuracy: 0.2781 - val_loss: -174946.0625 - val_accuracy: 0.3158\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -174023.5156 - accuracy: 0.2781 - val_loss: -175181.9531 - val_accuracy: 0.3158\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -174259.9062 - accuracy: 0.2781 - val_loss: -175418.0938 - val_accuracy: 0.3158\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -174496.5625 - accuracy: 0.2781 - val_loss: -175654.4219 - val_accuracy: 0.3158\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -174733.4219 - accuracy: 0.2781 - val_loss: -175891.0000 - val_accuracy: 0.3158\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -174970.5312 - accuracy: 0.2781 - val_loss: -176127.7812 - val_accuracy: 0.3158\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -175207.8750 - accuracy: 0.2781 - val_loss: -176364.8281 - val_accuracy: 0.3158\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: -175445.4531 - accuracy: 0.2781 - val_loss: -176602.0938 - val_accuracy: 0.3158\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -175683.2656 - accuracy: 0.2781 - val_loss: -176839.6250 - val_accuracy: 0.3158\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -175921.2969 - accuracy: 0.2781 - val_loss: -177077.3438 - val_accuracy: 0.3158\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -176159.5938 - accuracy: 0.2781 - val_loss: -177315.3125 - val_accuracy: 0.3158\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -176398.1250 - accuracy: 0.2781 - val_loss: -177553.4688 - val_accuracy: 0.3158\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -176636.8594 - accuracy: 0.2781 - val_loss: -177791.8750 - val_accuracy: 0.3158\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -176875.8594 - accuracy: 0.2781 - val_loss: -178030.4688 - val_accuracy: 0.3158\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -177115.0781 - accuracy: 0.2781 - val_loss: -178269.3281 - val_accuracy: 0.3158\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -177354.5312 - accuracy: 0.2781 - val_loss: -178508.4062 - val_accuracy: 0.3158\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -177594.2344 - accuracy: 0.2781 - val_loss: -178747.6875 - val_accuracy: 0.3158\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -177834.1719 - accuracy: 0.2781 - val_loss: -178987.2188 - val_accuracy: 0.3158\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: -178074.3438 - accuracy: 0.2781 - val_loss: -179226.9844 - val_accuracy: 0.3158\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -178314.7344 - accuracy: 0.2781 - val_loss: -179466.9688 - val_accuracy: 0.3158\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -178555.3750 - accuracy: 0.2781 - val_loss: -179707.2031 - val_accuracy: 0.3158\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -178796.2656 - accuracy: 0.2781 - val_loss: -179947.6250 - val_accuracy: 0.3158\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -179037.3594 - accuracy: 0.2781 - val_loss: -180188.2969 - val_accuracy: 0.3158\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -179278.7031 - accuracy: 0.2781 - val_loss: -180429.1719 - val_accuracy: 0.3158\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: -179520.2812 - accuracy: 0.2781 - val_loss: -180670.2812 - val_accuracy: 0.3158\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -179762.0781 - accuracy: 0.2781 - val_loss: -180911.6250 - val_accuracy: 0.3158\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -180004.1250 - accuracy: 0.2781 - val_loss: -181153.2344 - val_accuracy: 0.3158\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -180246.4219 - accuracy: 0.2781 - val_loss: -181395.0469 - val_accuracy: 0.3158\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -180488.9219 - accuracy: 0.2781 - val_loss: -181637.0781 - val_accuracy: 0.3158\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -180731.6875 - accuracy: 0.2781 - val_loss: -181879.3750 - val_accuracy: 0.3158\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -180974.6719 - accuracy: 0.2781 - val_loss: -182121.8594 - val_accuracy: 0.3158\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -181217.9219 - accuracy: 0.2781 - val_loss: -182364.5469 - val_accuracy: 0.3158\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -181461.3750 - accuracy: 0.2781 - val_loss: -182607.4844 - val_accuracy: 0.3158\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -181705.0781 - accuracy: 0.2781 - val_loss: -182850.6250 - val_accuracy: 0.3158\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: -181949.0156 - accuracy: 0.2781 - val_loss: -183094.0156 - val_accuracy: 0.3158\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 0s 91ms/step - loss: -182193.1875 - accuracy: 0.2781 - val_loss: -183337.6719 - val_accuracy: 0.3158\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: -182437.5781 - accuracy: 0.2781 - val_loss: -183581.5469 - val_accuracy: 0.3158\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -182682.2188 - accuracy: 0.2781 - val_loss: -183825.6094 - val_accuracy: 0.3158\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -182927.1094 - accuracy: 0.2781 - val_loss: -184069.9531 - val_accuracy: 0.3158\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -183172.2344 - accuracy: 0.2781 - val_loss: -184314.4531 - val_accuracy: 0.3158\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -183417.5938 - accuracy: 0.2781 - val_loss: -184559.2188 - val_accuracy: 0.3158\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: -183663.1719 - accuracy: 0.2781 - val_loss: -184804.1875 - val_accuracy: 0.3158\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: -183909.0000 - accuracy: 0.2781 - val_loss: -185049.4531 - val_accuracy: 0.3158\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: -184155.0469 - accuracy: 0.2781 - val_loss: -185294.9531 - val_accuracy: 0.3158\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: -184401.3750 - accuracy: 0.2781 - val_loss: -185540.6562 - val_accuracy: 0.3158\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: -184647.9219 - accuracy: 0.2781 - val_loss: -185786.5938 - val_accuracy: 0.3158\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: -184894.6719 - accuracy: 0.2781 - val_loss: -186032.7500 - val_accuracy: 0.3158\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -185141.6875 - accuracy: 0.2781 - val_loss: -186279.1719 - val_accuracy: 0.3158\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: -185388.9375 - accuracy: 0.2781 - val_loss: -186525.7969 - val_accuracy: 0.3158\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 95ms/step - loss: -185636.4375 - accuracy: 0.2781 - val_loss: -186772.6562 - val_accuracy: 0.3158\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: -185884.1406 - accuracy: 0.2781 - val_loss: -187019.7188 - val_accuracy: 0.3158\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -186132.1406 - accuracy: 0.2781 - val_loss: -187267.0312 - val_accuracy: 0.3158\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: -186380.3594 - accuracy: 0.2781 - val_loss: -187514.5781 - val_accuracy: 0.3158\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: -186628.8594 - accuracy: 0.2781 - val_loss: -187762.3750 - val_accuracy: 0.3158\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -186877.5625 - accuracy: 0.2781 - val_loss: -188010.4688 - val_accuracy: 0.3158\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -187126.5156 - accuracy: 0.2781 - val_loss: -188258.7969 - val_accuracy: 0.3158\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -187375.7188 - accuracy: 0.2781 - val_loss: -188507.3438 - val_accuracy: 0.3158\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: -187625.1406 - accuracy: 0.2781 - val_loss: -188756.1250 - val_accuracy: 0.3158\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: -187874.8125 - accuracy: 0.2781 - val_loss: -189005.1094 - val_accuracy: 0.3158\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -188124.7500 - accuracy: 0.2781 - val_loss: -189254.3438 - val_accuracy: 0.3158\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: -188374.9062 - accuracy: 0.2781 - val_loss: -189503.7969 - val_accuracy: 0.3158\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -188625.3281 - accuracy: 0.2781 - val_loss: -189753.4844 - val_accuracy: 0.3158\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -188875.9375 - accuracy: 0.2781 - val_loss: -190003.3906 - val_accuracy: 0.3158\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: -189126.8281 - accuracy: 0.2781 - val_loss: -190253.5469 - val_accuracy: 0.3158\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -189377.9844 - accuracy: 0.2781 - val_loss: -190503.9531 - val_accuracy: 0.3158\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -189629.3438 - accuracy: 0.2781 - val_loss: -190754.5625 - val_accuracy: 0.3158\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 91ms/step - loss: -189880.9531 - accuracy: 0.2781 - val_loss: -191005.3906 - val_accuracy: 0.3158\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: -190132.8125 - accuracy: 0.2781 - val_loss: -191256.4688 - val_accuracy: 0.3158\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -190384.9062 - accuracy: 0.2781 - val_loss: -191507.7812 - val_accuracy: 0.3158\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -190637.2500 - accuracy: 0.2781 - val_loss: -191759.2969 - val_accuracy: 0.3158\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -190889.7969 - accuracy: 0.2781 - val_loss: -192011.0625 - val_accuracy: 0.3158\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: -191142.6250 - accuracy: 0.2781 - val_loss: -192263.1094 - val_accuracy: 0.3158\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: -191395.6719 - accuracy: 0.2781 - val_loss: -192515.3906 - val_accuracy: 0.3158\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: -191649.0000 - accuracy: 0.2781 - val_loss: -192767.9531 - val_accuracy: 0.3158\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: -191902.5156 - accuracy: 0.2781 - val_loss: -193020.7031 - val_accuracy: 0.3158\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -192156.2812 - accuracy: 0.2781 - val_loss: -193273.7031 - val_accuracy: 0.3158\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -192410.3125 - accuracy: 0.2781 - val_loss: -193526.9062 - val_accuracy: 0.3158\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -192664.5781 - accuracy: 0.2781 - val_loss: -193780.3438 - val_accuracy: 0.3158\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -192919.0625 - accuracy: 0.2781 - val_loss: -194034.0000 - val_accuracy: 0.3158\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -193173.8281 - accuracy: 0.2781 - val_loss: -194287.8906 - val_accuracy: 0.3158\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -193428.8125 - accuracy: 0.2781 - val_loss: -194542.0312 - val_accuracy: 0.3158\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -193684.0312 - accuracy: 0.2781 - val_loss: -194796.3750 - val_accuracy: 0.3158\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -193939.5000 - accuracy: 0.2781 - val_loss: -195050.9531 - val_accuracy: 0.3158\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -194195.1875 - accuracy: 0.2781 - val_loss: -195305.8125 - val_accuracy: 0.3158\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -194451.1406 - accuracy: 0.2781 - val_loss: -195560.9375 - val_accuracy: 0.3158\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -194707.3438 - accuracy: 0.2781 - val_loss: -195816.2500 - val_accuracy: 0.3158\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -194963.7656 - accuracy: 0.2781 - val_loss: -196071.8125 - val_accuracy: 0.3158\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -195220.4219 - accuracy: 0.2781 - val_loss: -196327.6094 - val_accuracy: 0.3158\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -195477.3281 - accuracy: 0.2781 - val_loss: -196583.6250 - val_accuracy: 0.3158\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -195734.4688 - accuracy: 0.2781 - val_loss: -196839.8594 - val_accuracy: 0.3158\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -195991.8750 - accuracy: 0.2781 - val_loss: -197096.3125 - val_accuracy: 0.3158\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -196249.5312 - accuracy: 0.2781 - val_loss: -197353.0000 - val_accuracy: 0.3158\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -196507.3906 - accuracy: 0.2781 - val_loss: -197609.9219 - val_accuracy: 0.3158\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -196765.4844 - accuracy: 0.2781 - val_loss: -197867.1094 - val_accuracy: 0.3158\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -197023.8594 - accuracy: 0.2781 - val_loss: -198124.5156 - val_accuracy: 0.3158\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -197282.4531 - accuracy: 0.2781 - val_loss: -198382.1875 - val_accuracy: 0.3158\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -197541.3125 - accuracy: 0.2781 - val_loss: -198640.0469 - val_accuracy: 0.3158\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -197800.3750 - accuracy: 0.2781 - val_loss: -198898.1406 - val_accuracy: 0.3158\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -198059.7031 - accuracy: 0.2781 - val_loss: -199156.5156 - val_accuracy: 0.3158\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -198319.2812 - accuracy: 0.2781 - val_loss: -199415.0938 - val_accuracy: 0.3158\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: -198579.1250 - accuracy: 0.2781 - val_loss: -199673.9219 - val_accuracy: 0.3158\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -198839.1406 - accuracy: 0.2781 - val_loss: -199932.9531 - val_accuracy: 0.3158\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -199099.4531 - accuracy: 0.2781 - val_loss: -200192.2031 - val_accuracy: 0.3158\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -199360.0000 - accuracy: 0.2781 - val_loss: -200451.7188 - val_accuracy: 0.3158\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -199620.7812 - accuracy: 0.2781 - val_loss: -200711.4844 - val_accuracy: 0.3158\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: -199881.7812 - accuracy: 0.2781 - val_loss: -200971.4688 - val_accuracy: 0.3158\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -200143.0312 - accuracy: 0.2781 - val_loss: -201231.6875 - val_accuracy: 0.3158\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -200404.5469 - accuracy: 0.2781 - val_loss: -201492.1250 - val_accuracy: 0.3158\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -200666.2969 - accuracy: 0.2781 - val_loss: -201752.7969 - val_accuracy: 0.3158\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -200928.2812 - accuracy: 0.2781 - val_loss: -202013.7344 - val_accuracy: 0.3158\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -201190.5312 - accuracy: 0.2781 - val_loss: -202274.9219 - val_accuracy: 0.3158\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -201453.0000 - accuracy: 0.2781 - val_loss: -202536.3438 - val_accuracy: 0.3158\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -201715.7344 - accuracy: 0.2781 - val_loss: -202798.0000 - val_accuracy: 0.3158\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -201978.7031 - accuracy: 0.2781 - val_loss: -203059.8594 - val_accuracy: 0.3158\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -202241.8750 - accuracy: 0.2781 - val_loss: -203321.9844 - val_accuracy: 0.3158\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -202505.3281 - accuracy: 0.2781 - val_loss: -203584.3750 - val_accuracy: 0.3158\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -202769.0156 - accuracy: 0.2781 - val_loss: -203846.9844 - val_accuracy: 0.3158\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -203032.9531 - accuracy: 0.2781 - val_loss: -204109.8438 - val_accuracy: 0.3158\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -203297.1406 - accuracy: 0.2781 - val_loss: -204372.8906 - val_accuracy: 0.3158\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -203561.5625 - accuracy: 0.2781 - val_loss: -204636.2031 - val_accuracy: 0.3158\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -203826.2188 - accuracy: 0.2781 - val_loss: -204899.7031 - val_accuracy: 0.3158\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -204091.1406 - accuracy: 0.2781 - val_loss: -205163.4688 - val_accuracy: 0.3158\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -204356.3125 - accuracy: 0.2781 - val_loss: -205427.4219 - val_accuracy: 0.3158\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -204621.7031 - accuracy: 0.2781 - val_loss: -205691.7344 - val_accuracy: 0.3158\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: -204887.3438 - accuracy: 0.2781 - val_loss: -205956.2656 - val_accuracy: 0.3158\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -205153.2344 - accuracy: 0.2781 - val_loss: -206220.9844 - val_accuracy: 0.3158\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -205419.3125 - accuracy: 0.2781 - val_loss: -206485.9375 - val_accuracy: 0.3158\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -205685.7344 - accuracy: 0.2781 - val_loss: -206751.1406 - val_accuracy: 0.3158\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -205952.3594 - accuracy: 0.2781 - val_loss: -207016.5469 - val_accuracy: 0.3158\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -206219.2188 - accuracy: 0.2781 - val_loss: -207282.2188 - val_accuracy: 0.3158\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -206486.3125 - accuracy: 0.2781 - val_loss: -207548.1250 - val_accuracy: 0.3158\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -206753.6562 - accuracy: 0.2781 - val_loss: -207814.2500 - val_accuracy: 0.3158\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -207021.2812 - accuracy: 0.2781 - val_loss: -208080.6094 - val_accuracy: 0.3158\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: -207289.1094 - accuracy: 0.2781 - val_loss: -208347.2031 - val_accuracy: 0.3158\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -207557.2031 - accuracy: 0.2781 - val_loss: -208614.0781 - val_accuracy: 0.3158\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -207825.5312 - accuracy: 0.2781 - val_loss: -208881.1875 - val_accuracy: 0.3158\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -208094.1094 - accuracy: 0.2781 - val_loss: -209148.4844 - val_accuracy: 0.3158\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -208362.9375 - accuracy: 0.2781 - val_loss: -209416.0312 - val_accuracy: 0.3158\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -208631.9688 - accuracy: 0.2781 - val_loss: -209683.7969 - val_accuracy: 0.3158\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -208901.2812 - accuracy: 0.2781 - val_loss: -209951.7969 - val_accuracy: 0.3158\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -209170.8594 - accuracy: 0.2781 - val_loss: -210220.0156 - val_accuracy: 0.3158\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -209440.6406 - accuracy: 0.2781 - val_loss: -210488.5312 - val_accuracy: 0.3158\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -209710.7031 - accuracy: 0.2781 - val_loss: -210757.2656 - val_accuracy: 0.3158\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -209981.0000 - accuracy: 0.2781 - val_loss: -211026.2656 - val_accuracy: 0.3158\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: -210251.5312 - accuracy: 0.2781 - val_loss: -211295.4375 - val_accuracy: 0.3158\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -210522.3125 - accuracy: 0.2781 - val_loss: -211564.8750 - val_accuracy: 0.3158\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -210793.3750 - accuracy: 0.2781 - val_loss: -211834.5312 - val_accuracy: 0.3158\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -211064.6094 - accuracy: 0.2781 - val_loss: -212104.4219 - val_accuracy: 0.3158\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -211336.1719 - accuracy: 0.2781 - val_loss: -212374.5781 - val_accuracy: 0.3158\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -211607.9062 - accuracy: 0.2781 - val_loss: -212644.9531 - val_accuracy: 0.3158\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -211879.9219 - accuracy: 0.2781 - val_loss: -212915.5781 - val_accuracy: 0.3158\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -212152.1719 - accuracy: 0.2781 - val_loss: -213186.4375 - val_accuracy: 0.3158\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -212424.7031 - accuracy: 0.2781 - val_loss: -213457.5312 - val_accuracy: 0.3158\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -212697.4688 - accuracy: 0.2781 - val_loss: -213728.8438 - val_accuracy: 0.3158\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -212970.4688 - accuracy: 0.2781 - val_loss: -214000.4219 - val_accuracy: 0.3158\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -213243.7188 - accuracy: 0.2781 - val_loss: -214272.2188 - val_accuracy: 0.3158\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -213517.1875 - accuracy: 0.2781 - val_loss: -214544.2656 - val_accuracy: 0.3158\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -213790.9062 - accuracy: 0.2781 - val_loss: -214816.5312 - val_accuracy: 0.3158\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -214064.9062 - accuracy: 0.2781 - val_loss: -215089.0469 - val_accuracy: 0.3158\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -214339.1406 - accuracy: 0.2781 - val_loss: -215361.7969 - val_accuracy: 0.3158\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -214613.6406 - accuracy: 0.2781 - val_loss: -215634.7812 - val_accuracy: 0.3158\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -214888.3750 - accuracy: 0.2781 - val_loss: -215908.0000 - val_accuracy: 0.3158\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -215163.3594 - accuracy: 0.2781 - val_loss: -216181.4844 - val_accuracy: 0.3158\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -215438.6094 - accuracy: 0.2781 - val_loss: -216455.2188 - val_accuracy: 0.3158\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -215714.1094 - accuracy: 0.2781 - val_loss: -216729.2500 - val_accuracy: 0.3158\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -215989.8281 - accuracy: 0.2781 - val_loss: -217003.5000 - val_accuracy: 0.3158\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -216265.8281 - accuracy: 0.2781 - val_loss: -217277.9688 - val_accuracy: 0.3158\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -216542.0312 - accuracy: 0.2781 - val_loss: -217552.7031 - val_accuracy: 0.3158\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -216818.5625 - accuracy: 0.2781 - val_loss: -217827.6562 - val_accuracy: 0.3158\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -217095.2969 - accuracy: 0.2781 - val_loss: -218102.8594 - val_accuracy: 0.3158\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -217372.2656 - accuracy: 0.2781 - val_loss: -218378.2969 - val_accuracy: 0.3158\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -217649.4844 - accuracy: 0.2781 - val_loss: -218654.0000 - val_accuracy: 0.3158\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -217926.9844 - accuracy: 0.2781 - val_loss: -218929.9219 - val_accuracy: 0.3158\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -218204.7344 - accuracy: 0.2781 - val_loss: -219206.0469 - val_accuracy: 0.3158\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -218482.7031 - accuracy: 0.2781 - val_loss: -219482.4531 - val_accuracy: 0.3158\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -218760.9219 - accuracy: 0.2781 - val_loss: -219759.1250 - val_accuracy: 0.3158\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -219039.4062 - accuracy: 0.2781 - val_loss: -220035.9531 - val_accuracy: 0.3158\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -219318.1250 - accuracy: 0.2781 - val_loss: -220313.0781 - val_accuracy: 0.3158\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -219597.1094 - accuracy: 0.2781 - val_loss: -220590.4531 - val_accuracy: 0.3158\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -219876.3594 - accuracy: 0.2781 - val_loss: -220868.0469 - val_accuracy: 0.3158\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -220155.7812 - accuracy: 0.2781 - val_loss: -221145.9531 - val_accuracy: 0.3158\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -220435.5000 - accuracy: 0.2781 - val_loss: -221424.0469 - val_accuracy: 0.3158\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -220715.5000 - accuracy: 0.2781 - val_loss: -221702.3750 - val_accuracy: 0.3158\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -220995.6875 - accuracy: 0.2781 - val_loss: -221980.9531 - val_accuracy: 0.3158\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -221276.1719 - accuracy: 0.2781 - val_loss: -222259.7656 - val_accuracy: 0.3158\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -221556.8750 - accuracy: 0.2781 - val_loss: -222538.8438 - val_accuracy: 0.3158\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -221837.8281 - accuracy: 0.2781 - val_loss: -222818.1562 - val_accuracy: 0.3158\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -222119.0469 - accuracy: 0.2781 - val_loss: -223097.7031 - val_accuracy: 0.3158\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -222400.5156 - accuracy: 0.2781 - val_loss: -223377.5000 - val_accuracy: 0.3158\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -222682.2188 - accuracy: 0.2781 - val_loss: -223657.4688 - val_accuracy: 0.3158\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -222964.1719 - accuracy: 0.2781 - val_loss: -223937.7031 - val_accuracy: 0.3158\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -223246.3906 - accuracy: 0.2781 - val_loss: -224218.2031 - val_accuracy: 0.3158\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -223528.8594 - accuracy: 0.2781 - val_loss: -224498.9688 - val_accuracy: 0.3158\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -223811.5781 - accuracy: 0.2781 - val_loss: -224779.9688 - val_accuracy: 0.3158\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -224094.5312 - accuracy: 0.2781 - val_loss: -225061.2344 - val_accuracy: 0.3158\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -224377.7812 - accuracy: 0.2781 - val_loss: -225342.7344 - val_accuracy: 0.3158\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -224661.2031 - accuracy: 0.2781 - val_loss: -225624.4531 - val_accuracy: 0.3158\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -224944.9531 - accuracy: 0.2781 - val_loss: -225906.3906 - val_accuracy: 0.3158\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -225228.9219 - accuracy: 0.2781 - val_loss: -226188.5469 - val_accuracy: 0.3158\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -225513.1562 - accuracy: 0.2781 - val_loss: -226470.9688 - val_accuracy: 0.3158\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -225797.6562 - accuracy: 0.2781 - val_loss: -226753.6562 - val_accuracy: 0.3158\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -226082.3906 - accuracy: 0.2781 - val_loss: -227036.5781 - val_accuracy: 0.3158\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -226367.3594 - accuracy: 0.2781 - val_loss: -227319.7969 - val_accuracy: 0.3158\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -226652.5938 - accuracy: 0.2781 - val_loss: -227603.1875 - val_accuracy: 0.3158\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -226938.0781 - accuracy: 0.2781 - val_loss: -227886.8125 - val_accuracy: 0.3158\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -227223.8125 - accuracy: 0.2781 - val_loss: -228170.6875 - val_accuracy: 0.3158\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -227509.7969 - accuracy: 0.2781 - val_loss: -228454.8438 - val_accuracy: 0.3158\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -227796.0312 - accuracy: 0.2781 - val_loss: -228739.2031 - val_accuracy: 0.3158\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -228082.5156 - accuracy: 0.2781 - val_loss: -229023.8438 - val_accuracy: 0.3158\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -228369.2500 - accuracy: 0.2781 - val_loss: -229308.7344 - val_accuracy: 0.3158\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -228656.2344 - accuracy: 0.2781 - val_loss: -229593.8125 - val_accuracy: 0.3158\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -228943.4844 - accuracy: 0.2781 - val_loss: -229879.1875 - val_accuracy: 0.3158\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -229230.9531 - accuracy: 0.2781 - val_loss: -230164.7656 - val_accuracy: 0.3158\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -229518.7031 - accuracy: 0.2781 - val_loss: -230450.5781 - val_accuracy: 0.3158\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -229806.7031 - accuracy: 0.2781 - val_loss: -230736.5781 - val_accuracy: 0.3158\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -230094.9375 - accuracy: 0.2781 - val_loss: -231022.8750 - val_accuracy: 0.3158\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -230383.4375 - accuracy: 0.2781 - val_loss: -231309.4219 - val_accuracy: 0.3158\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -230672.1875 - accuracy: 0.2781 - val_loss: -231596.2656 - val_accuracy: 0.3158\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -230961.1875 - accuracy: 0.2781 - val_loss: -231883.2969 - val_accuracy: 0.3158\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -231250.4219 - accuracy: 0.2781 - val_loss: -232170.5781 - val_accuracy: 0.3158\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -231539.9531 - accuracy: 0.2781 - val_loss: -232458.0781 - val_accuracy: 0.3158\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -231829.7031 - accuracy: 0.2781 - val_loss: -232745.8125 - val_accuracy: 0.3158\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -232119.7188 - accuracy: 0.2781 - val_loss: -233033.7969 - val_accuracy: 0.3158\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -232409.9375 - accuracy: 0.2781 - val_loss: -233322.0000 - val_accuracy: 0.3158\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -232700.4531 - accuracy: 0.2781 - val_loss: -233610.4688 - val_accuracy: 0.3158\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -232991.2188 - accuracy: 0.2781 - val_loss: -233899.2656 - val_accuracy: 0.3158\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -233282.2188 - accuracy: 0.2781 - val_loss: -234188.2969 - val_accuracy: 0.3158\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -233573.5156 - accuracy: 0.2781 - val_loss: -234477.5469 - val_accuracy: 0.3158\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: -233865.0156 - accuracy: 0.2781 - val_loss: -234767.0000 - val_accuracy: 0.3158\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -234156.7812 - accuracy: 0.2781 - val_loss: -235056.7969 - val_accuracy: 0.3158\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -234448.8125 - accuracy: 0.2781 - val_loss: -235346.7969 - val_accuracy: 0.3158\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -234741.0469 - accuracy: 0.2781 - val_loss: -235637.0000 - val_accuracy: 0.3158\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -235033.5781 - accuracy: 0.2781 - val_loss: -235927.4219 - val_accuracy: 0.3158\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -235326.3438 - accuracy: 0.2781 - val_loss: -236218.0781 - val_accuracy: 0.3158\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -235619.3906 - accuracy: 0.2781 - val_loss: -236509.0000 - val_accuracy: 0.3158\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -235912.6875 - accuracy: 0.2781 - val_loss: -236800.1094 - val_accuracy: 0.3158\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -236206.2031 - accuracy: 0.2781 - val_loss: -237091.4688 - val_accuracy: 0.3158\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -236500.0000 - accuracy: 0.2781 - val_loss: -237383.0781 - val_accuracy: 0.3158\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -236794.0312 - accuracy: 0.2781 - val_loss: -237674.9688 - val_accuracy: 0.3158\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -237088.3281 - accuracy: 0.2781 - val_loss: -237967.1875 - val_accuracy: 0.3158\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -237382.8906 - accuracy: 0.2781 - val_loss: -238259.6094 - val_accuracy: 0.3158\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -237677.7031 - accuracy: 0.2781 - val_loss: -238552.2969 - val_accuracy: 0.3158\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -237972.7500 - accuracy: 0.2781 - val_loss: -238845.2031 - val_accuracy: 0.3158\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -238268.0469 - accuracy: 0.2781 - val_loss: -239138.3125 - val_accuracy: 0.3158\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -238563.6250 - accuracy: 0.2781 - val_loss: -239431.6875 - val_accuracy: 0.3158\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -238859.4062 - accuracy: 0.2781 - val_loss: -239725.2969 - val_accuracy: 0.3158\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -239155.5000 - accuracy: 0.2781 - val_loss: -240019.1094 - val_accuracy: 0.3158\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -239451.8281 - accuracy: 0.2781 - val_loss: -240313.1875 - val_accuracy: 0.3158\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -239748.4219 - accuracy: 0.2781 - val_loss: -240607.4688 - val_accuracy: 0.3158\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -240045.2344 - accuracy: 0.2781 - val_loss: -240902.0312 - val_accuracy: 0.3158\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -240342.3125 - accuracy: 0.2781 - val_loss: -241196.7969 - val_accuracy: 0.3158\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -240639.6719 - accuracy: 0.2781 - val_loss: -241491.8438 - val_accuracy: 0.3158\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -240937.2500 - accuracy: 0.2781 - val_loss: -241787.1875 - val_accuracy: 0.3158\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -241235.0469 - accuracy: 0.2781 - val_loss: -242082.7969 - val_accuracy: 0.3158\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -241533.1875 - accuracy: 0.2781 - val_loss: -242378.6250 - val_accuracy: 0.3158\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: -241831.5312 - accuracy: 0.2781 - val_loss: -242674.6875 - val_accuracy: 0.3158\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: -242130.1562 - accuracy: 0.2781 - val_loss: -242971.0000 - val_accuracy: 0.3158\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: -242428.9688 - accuracy: 0.2781 - val_loss: -243267.5312 - val_accuracy: 0.3158\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: -242728.0938 - accuracy: 0.2781 - val_loss: -243564.2344 - val_accuracy: 0.3158\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -243027.4844 - accuracy: 0.2781 - val_loss: -243861.2031 - val_accuracy: 0.3158\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -243327.0781 - accuracy: 0.2781 - val_loss: -244158.4688 - val_accuracy: 0.3158\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: -243626.9531 - accuracy: 0.2781 - val_loss: -244455.9531 - val_accuracy: 0.3158\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: -243927.1250 - accuracy: 0.2781 - val_loss: -244753.6250 - val_accuracy: 0.3158\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -244227.5000 - accuracy: 0.2781 - val_loss: -245051.6875 - val_accuracy: 0.3158\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -244528.1406 - accuracy: 0.2781 - val_loss: -245349.9219 - val_accuracy: 0.3158\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -244829.0469 - accuracy: 0.2781 - val_loss: -245648.3906 - val_accuracy: 0.3158\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -245130.2031 - accuracy: 0.2781 - val_loss: -245947.1094 - val_accuracy: 0.3158\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -245431.5938 - accuracy: 0.2781 - val_loss: -246246.0781 - val_accuracy: 0.3158\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: -245733.2812 - accuracy: 0.2781 - val_loss: -246545.2656 - val_accuracy: 0.3158\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -246035.1719 - accuracy: 0.2781 - val_loss: -246844.7656 - val_accuracy: 0.3158\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -246337.3281 - accuracy: 0.2781 - val_loss: -247144.5312 - val_accuracy: 0.3158\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -246639.7656 - accuracy: 0.2781 - val_loss: -247444.5781 - val_accuracy: 0.3158\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -246942.4375 - accuracy: 0.2781 - val_loss: -247744.7969 - val_accuracy: 0.3158\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: -247245.3750 - accuracy: 0.2781 - val_loss: -248045.2969 - val_accuracy: 0.3158\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 95ms/step - loss: -247548.5938 - accuracy: 0.2781 - val_loss: -248346.0000 - val_accuracy: 0.3158\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: -247852.0312 - accuracy: 0.2781 - val_loss: -248646.9531 - val_accuracy: 0.3158\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -248155.7188 - accuracy: 0.2781 - val_loss: -248948.1562 - val_accuracy: 0.3158\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: -248459.6719 - accuracy: 0.2781 - val_loss: -249249.6562 - val_accuracy: 0.3158\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -248763.8594 - accuracy: 0.2781 - val_loss: -249551.4219 - val_accuracy: 0.3158\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -249068.3281 - accuracy: 0.2781 - val_loss: -249853.4531 - val_accuracy: 0.3158\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -249373.0469 - accuracy: 0.2781 - val_loss: -250155.6250 - val_accuracy: 0.3158\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -249678.0156 - accuracy: 0.2781 - val_loss: -250458.1094 - val_accuracy: 0.3158\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: -249983.2656 - accuracy: 0.2781 - val_loss: -250760.8125 - val_accuracy: 0.3158\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: -250288.7500 - accuracy: 0.2781 - val_loss: -251063.8750 - val_accuracy: 0.3158\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -250594.5156 - accuracy: 0.2781 - val_loss: -251367.1250 - val_accuracy: 0.3158\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -250900.5156 - accuracy: 0.2781 - val_loss: -251670.6875 - val_accuracy: 0.3158\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: -251206.7656 - accuracy: 0.2781 - val_loss: -251974.4219 - val_accuracy: 0.3158\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: -251513.2969 - accuracy: 0.2781 - val_loss: -252278.4219 - val_accuracy: 0.3158\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: -251820.1250 - accuracy: 0.2781 - val_loss: -252582.6562 - val_accuracy: 0.3158\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: -252127.1406 - accuracy: 0.2781 - val_loss: -252887.0781 - val_accuracy: 0.3158\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: -252434.4375 - accuracy: 0.2781 - val_loss: -253191.7969 - val_accuracy: 0.3158\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: -252742.0156 - accuracy: 0.2781 - val_loss: -253496.7031 - val_accuracy: 0.3158\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 0s 95ms/step - loss: -253049.8438 - accuracy: 0.2781 - val_loss: -253801.9219 - val_accuracy: 0.3158\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 92ms/step - loss: -253357.9219 - accuracy: 0.2781 - val_loss: -254107.3438 - val_accuracy: 0.3158\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -253666.2500 - accuracy: 0.2781 - val_loss: -254413.0469 - val_accuracy: 0.3158\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: -253974.8594 - accuracy: 0.2781 - val_loss: -254719.0781 - val_accuracy: 0.3158\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -254283.7188 - accuracy: 0.2781 - val_loss: -255025.3750 - val_accuracy: 0.3158\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -254592.8281 - accuracy: 0.2781 - val_loss: -255331.8438 - val_accuracy: 0.3158\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -254902.2031 - accuracy: 0.2781 - val_loss: -255638.5469 - val_accuracy: 0.3158\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -255211.8281 - accuracy: 0.2781 - val_loss: -255945.5312 - val_accuracy: 0.3158\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 98ms/step - loss: -255521.7500 - accuracy: 0.2781 - val_loss: -256252.7969 - val_accuracy: 0.3158\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: -255831.8750 - accuracy: 0.2781 - val_loss: -256560.3125 - val_accuracy: 0.3158\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: -256142.2969 - accuracy: 0.2781 - val_loss: -256868.0469 - val_accuracy: 0.3158\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: -256452.9531 - accuracy: 0.2781 - val_loss: -257176.0312 - val_accuracy: 0.3158\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -256763.8750 - accuracy: 0.2781 - val_loss: -257484.2969 - val_accuracy: 0.3158\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -257075.0781 - accuracy: 0.2781 - val_loss: -257792.7969 - val_accuracy: 0.3158\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: -257386.5156 - accuracy: 0.2781 - val_loss: -258101.5469 - val_accuracy: 0.3158\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -257698.2031 - accuracy: 0.2781 - val_loss: -258410.5312 - val_accuracy: 0.3158\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -258010.1250 - accuracy: 0.2781 - val_loss: -258719.8125 - val_accuracy: 0.3158\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: -258322.3594 - accuracy: 0.2781 - val_loss: -259029.3438 - val_accuracy: 0.3158\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -258634.8125 - accuracy: 0.2781 - val_loss: -259339.1562 - val_accuracy: 0.3158\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -258947.5469 - accuracy: 0.2781 - val_loss: -259649.1250 - val_accuracy: 0.3158\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -259260.5469 - accuracy: 0.2781 - val_loss: -259959.3750 - val_accuracy: 0.3158\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -259573.7500 - accuracy: 0.2781 - val_loss: -260269.8438 - val_accuracy: 0.3158\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -259887.2500 - accuracy: 0.2781 - val_loss: -260580.6094 - val_accuracy: 0.3158\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -260201.0156 - accuracy: 0.2781 - val_loss: -260891.5781 - val_accuracy: 0.3158\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: -260515.0000 - accuracy: 0.2781 - val_loss: -261202.8750 - val_accuracy: 0.3158\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -260829.2969 - accuracy: 0.2781 - val_loss: -261514.4531 - val_accuracy: 0.3158\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -261143.8125 - accuracy: 0.2781 - val_loss: -261826.2344 - val_accuracy: 0.3158\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -261458.6094 - accuracy: 0.2781 - val_loss: -262138.2344 - val_accuracy: 0.3158\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -261773.6406 - accuracy: 0.2781 - val_loss: -262450.5312 - val_accuracy: 0.3158\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -262088.9531 - accuracy: 0.2781 - val_loss: -262763.0000 - val_accuracy: 0.3158\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: -262404.5312 - accuracy: 0.2781 - val_loss: -263075.7500 - val_accuracy: 0.3158\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -262720.3750 - accuracy: 0.2781 - val_loss: -263388.7188 - val_accuracy: 0.3158\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -263036.4375 - accuracy: 0.2781 - val_loss: -263701.9062 - val_accuracy: 0.3158\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -263352.7812 - accuracy: 0.2781 - val_loss: -264015.3750 - val_accuracy: 0.3158\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: -263669.3750 - accuracy: 0.2781 - val_loss: -264329.0625 - val_accuracy: 0.3158\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -263986.2500 - accuracy: 0.2781 - val_loss: -264643.0625 - val_accuracy: 0.3158\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -264303.3438 - accuracy: 0.2781 - val_loss: -264957.2500 - val_accuracy: 0.3158\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: -264620.7188 - accuracy: 0.2781 - val_loss: -265271.7812 - val_accuracy: 0.3158\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -264938.3438 - accuracy: 0.2781 - val_loss: -265586.6562 - val_accuracy: 0.3158\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -265256.2500 - accuracy: 0.2781 - val_loss: -265901.7812 - val_accuracy: 0.3158\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: -265574.4062 - accuracy: 0.2781 - val_loss: -266217.0938 - val_accuracy: 0.3158\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -265892.7812 - accuracy: 0.2781 - val_loss: -266532.6250 - val_accuracy: 0.3158\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -266211.4688 - accuracy: 0.2781 - val_loss: -266848.4062 - val_accuracy: 0.3158\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -266530.3750 - accuracy: 0.2781 - val_loss: -267164.4062 - val_accuracy: 0.3158\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -266849.6250 - accuracy: 0.2781 - val_loss: -267480.6250 - val_accuracy: 0.3158\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -267169.0312 - accuracy: 0.2781 - val_loss: -267797.0625 - val_accuracy: 0.3158\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -267488.7500 - accuracy: 0.2781 - val_loss: -268113.7500 - val_accuracy: 0.3158\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -267808.7188 - accuracy: 0.2781 - val_loss: -268430.6875 - val_accuracy: 0.3158\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -268128.9375 - accuracy: 0.2781 - val_loss: -268747.9375 - val_accuracy: 0.3158\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -268449.4062 - accuracy: 0.2781 - val_loss: -269065.4062 - val_accuracy: 0.3158\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -268770.1250 - accuracy: 0.2781 - val_loss: -269383.2188 - val_accuracy: 0.3158\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -269091.1250 - accuracy: 0.2781 - val_loss: -269701.1250 - val_accuracy: 0.3158\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -269412.4062 - accuracy: 0.2781 - val_loss: -270019.3438 - val_accuracy: 0.3158\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -269733.9062 - accuracy: 0.2781 - val_loss: -270337.7812 - val_accuracy: 0.3158\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -270055.6875 - accuracy: 0.2781 - val_loss: -270656.5625 - val_accuracy: 0.3158\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -270377.7188 - accuracy: 0.2781 - val_loss: -270975.4688 - val_accuracy: 0.3158\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -270700.0312 - accuracy: 0.2781 - val_loss: -271294.7188 - val_accuracy: 0.3158\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -271022.5938 - accuracy: 0.2781 - val_loss: -271614.1562 - val_accuracy: 0.3158\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -271345.3750 - accuracy: 0.2781 - val_loss: -271933.9062 - val_accuracy: 0.3158\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -271668.4688 - accuracy: 0.2781 - val_loss: -272253.7812 - val_accuracy: 0.3158\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -271991.8125 - accuracy: 0.2781 - val_loss: -272574.0312 - val_accuracy: 0.3158\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -272315.4062 - accuracy: 0.2781 - val_loss: -272894.5312 - val_accuracy: 0.3158\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -272639.2812 - accuracy: 0.2781 - val_loss: -273215.2500 - val_accuracy: 0.3158\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -272963.3750 - accuracy: 0.2781 - val_loss: -273536.1562 - val_accuracy: 0.3158\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -273287.7500 - accuracy: 0.2781 - val_loss: -273857.3438 - val_accuracy: 0.3158\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -273612.3438 - accuracy: 0.2781 - val_loss: -274178.7500 - val_accuracy: 0.3158\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -273937.2812 - accuracy: 0.2781 - val_loss: -274500.3750 - val_accuracy: 0.3158\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -274262.4062 - accuracy: 0.2781 - val_loss: -274822.2812 - val_accuracy: 0.3158\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: -274587.7812 - accuracy: 0.2781 - val_loss: -275144.5625 - val_accuracy: 0.3158\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -274913.5000 - accuracy: 0.2781 - val_loss: -275467.0938 - val_accuracy: 0.3158\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: -275239.4375 - accuracy: 0.2781 - val_loss: -275789.8125 - val_accuracy: 0.3158\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -275565.6250 - accuracy: 0.2781 - val_loss: -276112.7500 - val_accuracy: 0.3158\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -275892.0625 - accuracy: 0.2781 - val_loss: -276435.9375 - val_accuracy: 0.3158\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: -276218.7812 - accuracy: 0.2781 - val_loss: -276759.3750 - val_accuracy: 0.3158\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -276545.7500 - accuracy: 0.2781 - val_loss: -277083.0625 - val_accuracy: 0.3158\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -276873.0312 - accuracy: 0.2781 - val_loss: -277406.9375 - val_accuracy: 0.3158\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -277200.5000 - accuracy: 0.2781 - val_loss: -277731.0938 - val_accuracy: 0.3158\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -277528.2500 - accuracy: 0.2781 - val_loss: -278055.5312 - val_accuracy: 0.3158\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: -277856.2812 - accuracy: 0.2781 - val_loss: -278380.1875 - val_accuracy: 0.3158\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -278184.5312 - accuracy: 0.2781 - val_loss: -278705.0938 - val_accuracy: 0.3158\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -278513.0938 - accuracy: 0.2781 - val_loss: -279030.2500 - val_accuracy: 0.3158\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -278841.9062 - accuracy: 0.2781 - val_loss: -279355.6250 - val_accuracy: 0.3158\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -279170.9375 - accuracy: 0.2781 - val_loss: -279681.3125 - val_accuracy: 0.3158\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -279500.2500 - accuracy: 0.2781 - val_loss: -280007.3438 - val_accuracy: 0.3158\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -279829.8438 - accuracy: 0.2781 - val_loss: -280333.5938 - val_accuracy: 0.3158\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -280159.6875 - accuracy: 0.2781 - val_loss: -280660.0938 - val_accuracy: 0.3158\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -280489.8125 - accuracy: 0.2781 - val_loss: -280986.8125 - val_accuracy: 0.3158\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -280820.1562 - accuracy: 0.2781 - val_loss: -281313.7500 - val_accuracy: 0.3158\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -281150.7812 - accuracy: 0.2781 - val_loss: -281640.9375 - val_accuracy: 0.3158\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -281481.6562 - accuracy: 0.2781 - val_loss: -281968.3125 - val_accuracy: 0.3158\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -281812.8438 - accuracy: 0.2781 - val_loss: -282296.0000 - val_accuracy: 0.3158\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -282144.2500 - accuracy: 0.2781 - val_loss: -282623.9062 - val_accuracy: 0.3158\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -282475.9062 - accuracy: 0.2781 - val_loss: -282952.0000 - val_accuracy: 0.3158\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: -282807.8125 - accuracy: 0.2781 - val_loss: -283280.3750 - val_accuracy: 0.3158\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -283140.0000 - accuracy: 0.2781 - val_loss: -283609.0000 - val_accuracy: 0.3158\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -283472.4375 - accuracy: 0.2781 - val_loss: -283937.9688 - val_accuracy: 0.3158\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -283805.1250 - accuracy: 0.2781 - val_loss: -284267.2188 - val_accuracy: 0.3158\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -284138.0938 - accuracy: 0.2781 - val_loss: -284596.7188 - val_accuracy: 0.3158\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -284471.3438 - accuracy: 0.2781 - val_loss: -284926.4375 - val_accuracy: 0.3158\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -284804.8125 - accuracy: 0.2781 - val_loss: -285256.4062 - val_accuracy: 0.3158\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -285138.5625 - accuracy: 0.2781 - val_loss: -285586.5938 - val_accuracy: 0.3158\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -285472.5625 - accuracy: 0.2781 - val_loss: -285917.0312 - val_accuracy: 0.3158\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -285806.8438 - accuracy: 0.2781 - val_loss: -286247.6875 - val_accuracy: 0.3158\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -286141.4062 - accuracy: 0.2781 - val_loss: -286578.6562 - val_accuracy: 0.3158\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -286476.1875 - accuracy: 0.2781 - val_loss: -286909.8438 - val_accuracy: 0.3158\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -286811.2812 - accuracy: 0.2781 - val_loss: -287241.3438 - val_accuracy: 0.3158\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: -287146.5938 - accuracy: 0.2781 - val_loss: -287573.0938 - val_accuracy: 0.3158\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -287482.2188 - accuracy: 0.2781 - val_loss: -287905.0938 - val_accuracy: 0.3158\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -287818.0625 - accuracy: 0.2781 - val_loss: -288237.3125 - val_accuracy: 0.3158\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -288154.1562 - accuracy: 0.2781 - val_loss: -288569.7812 - val_accuracy: 0.3158\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -288490.5625 - accuracy: 0.2781 - val_loss: -288902.5000 - val_accuracy: 0.3158\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -288827.1875 - accuracy: 0.2781 - val_loss: -289235.4062 - val_accuracy: 0.3158\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -289164.1250 - accuracy: 0.2781 - val_loss: -289568.6250 - val_accuracy: 0.3158\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -289501.3125 - accuracy: 0.2781 - val_loss: -289902.1562 - val_accuracy: 0.3158\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -289838.7188 - accuracy: 0.2781 - val_loss: -290235.9062 - val_accuracy: 0.3158\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -290176.4375 - accuracy: 0.2781 - val_loss: -290569.9062 - val_accuracy: 0.3158\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -290514.3750 - accuracy: 0.2781 - val_loss: -290904.1250 - val_accuracy: 0.3158\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -290852.5938 - accuracy: 0.2781 - val_loss: -291238.5938 - val_accuracy: 0.3158\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -291191.0938 - accuracy: 0.2781 - val_loss: -291573.3125 - val_accuracy: 0.3158\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: -291529.8125 - accuracy: 0.2781 - val_loss: -291908.2812 - val_accuracy: 0.3158\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -291868.8438 - accuracy: 0.2781 - val_loss: -292243.5000 - val_accuracy: 0.3158\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -292208.1250 - accuracy: 0.2781 - val_loss: -292579.0312 - val_accuracy: 0.3158\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: -292547.6562 - accuracy: 0.2781 - val_loss: -292914.7812 - val_accuracy: 0.3158\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: -292887.4688 - accuracy: 0.2781 - val_loss: -293250.7812 - val_accuracy: 0.3158\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -293227.5312 - accuracy: 0.2781 - val_loss: -293587.0312 - val_accuracy: 0.3158\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -293567.8750 - accuracy: 0.2781 - val_loss: -293923.5312 - val_accuracy: 0.3158\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -293908.4375 - accuracy: 0.2781 - val_loss: -294260.2500 - val_accuracy: 0.3158\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -294249.3125 - accuracy: 0.2781 - val_loss: -294597.2500 - val_accuracy: 0.3158\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -294590.4062 - accuracy: 0.2781 - val_loss: -294934.5312 - val_accuracy: 0.3158\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -294931.8125 - accuracy: 0.2781 - val_loss: -295272.0938 - val_accuracy: 0.3158\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: -295273.4062 - accuracy: 0.2781 - val_loss: -295609.8125 - val_accuracy: 0.3158\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -295615.2812 - accuracy: 0.2781 - val_loss: -295947.8125 - val_accuracy: 0.3158\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: -295957.4688 - accuracy: 0.2781 - val_loss: -296286.0938 - val_accuracy: 0.3158\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -296299.8750 - accuracy: 0.2781 - val_loss: -296624.5938 - val_accuracy: 0.3158\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -296642.5938 - accuracy: 0.2781 - val_loss: -296963.4688 - val_accuracy: 0.3158\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: -296985.5625 - accuracy: 0.2781 - val_loss: -297302.5312 - val_accuracy: 0.3158\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -297328.7188 - accuracy: 0.2781 - val_loss: -297641.8438 - val_accuracy: 0.3158\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -297672.1875 - accuracy: 0.2781 - val_loss: -297981.3750 - val_accuracy: 0.3158\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -298015.9375 - accuracy: 0.2781 - val_loss: -298321.1875 - val_accuracy: 0.3158\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -298359.9375 - accuracy: 0.2781 - val_loss: -298661.2188 - val_accuracy: 0.3158\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -298704.2500 - accuracy: 0.2781 - val_loss: -299001.4688 - val_accuracy: 0.3158\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -299048.7500 - accuracy: 0.2781 - val_loss: -299341.9688 - val_accuracy: 0.3158\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -299393.5312 - accuracy: 0.2781 - val_loss: -299682.8750 - val_accuracy: 0.3158\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -299738.5938 - accuracy: 0.2781 - val_loss: -300024.0000 - val_accuracy: 0.3158\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -300083.9375 - accuracy: 0.2781 - val_loss: -300365.3125 - val_accuracy: 0.3158\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -300429.5000 - accuracy: 0.2781 - val_loss: -300706.9062 - val_accuracy: 0.3158\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -300775.3750 - accuracy: 0.2781 - val_loss: -301048.7188 - val_accuracy: 0.3158\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -301121.5000 - accuracy: 0.2781 - val_loss: -301390.7812 - val_accuracy: 0.3158\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -301467.8438 - accuracy: 0.2781 - val_loss: -301733.0938 - val_accuracy: 0.3158\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: -301814.5000 - accuracy: 0.2781 - val_loss: -302075.6562 - val_accuracy: 0.3158\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -302161.4062 - accuracy: 0.2781 - val_loss: -302418.4375 - val_accuracy: 0.3158\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -302508.5625 - accuracy: 0.2781 - val_loss: -302761.5000 - val_accuracy: 0.3158\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -302856.0000 - accuracy: 0.2781 - val_loss: -303104.8750 - val_accuracy: 0.3158\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -303203.6875 - accuracy: 0.2781 - val_loss: -303448.4375 - val_accuracy: 0.3158\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -303551.6562 - accuracy: 0.2781 - val_loss: -303792.2500 - val_accuracy: 0.3158\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: -303900.0625 - accuracy: 0.2781 - val_loss: -304136.3125 - val_accuracy: 0.3158\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -304248.8125 - accuracy: 0.2781 - val_loss: -304480.7500 - val_accuracy: 0.3158\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -304597.9062 - accuracy: 0.2781 - val_loss: -304825.4375 - val_accuracy: 0.3158\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -304947.2812 - accuracy: 0.2781 - val_loss: -305170.2812 - val_accuracy: 0.3158\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -305296.9688 - accuracy: 0.2781 - val_loss: -305515.4375 - val_accuracy: 0.3158\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -305646.9688 - accuracy: 0.2781 - val_loss: -305860.8125 - val_accuracy: 0.3158\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -305997.2500 - accuracy: 0.2781 - val_loss: -306206.4375 - val_accuracy: 0.3158\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -306347.8438 - accuracy: 0.2781 - val_loss: -306552.2500 - val_accuracy: 0.3158\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -306698.7188 - accuracy: 0.2781 - val_loss: -306898.2812 - val_accuracy: 0.3158\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -307049.9062 - accuracy: 0.2781 - val_loss: -307244.5625 - val_accuracy: 0.3158\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -307401.3750 - accuracy: 0.2781 - val_loss: -307591.0625 - val_accuracy: 0.3158\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -307753.1250 - accuracy: 0.2781 - val_loss: -307937.8438 - val_accuracy: 0.3158\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -308105.1250 - accuracy: 0.2781 - val_loss: -308285.0312 - val_accuracy: 0.3158\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -308457.4375 - accuracy: 0.2781 - val_loss: -308632.4688 - val_accuracy: 0.3158\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -308810.0312 - accuracy: 0.2781 - val_loss: -308980.1875 - val_accuracy: 0.3158\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -309162.8750 - accuracy: 0.2781 - val_loss: -309328.0938 - val_accuracy: 0.3158\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -309516.0625 - accuracy: 0.2781 - val_loss: -309676.2500 - val_accuracy: 0.3158\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -309869.4688 - accuracy: 0.2781 - val_loss: -310024.6250 - val_accuracy: 0.3158\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -310223.1250 - accuracy: 0.2781 - val_loss: -310373.2812 - val_accuracy: 0.3158\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -310577.0938 - accuracy: 0.2781 - val_loss: -310722.1875 - val_accuracy: 0.3158\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -310931.3438 - accuracy: 0.2781 - val_loss: -311071.4062 - val_accuracy: 0.3158\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -311285.8438 - accuracy: 0.2781 - val_loss: -311420.8750 - val_accuracy: 0.3158\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: -311640.5938 - accuracy: 0.2781 - val_loss: -311770.5312 - val_accuracy: 0.3158\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -311995.6562 - accuracy: 0.2781 - val_loss: -312120.4062 - val_accuracy: 0.3158\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -312350.9688 - accuracy: 0.2781 - val_loss: -312470.5938 - val_accuracy: 0.3158\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -312706.5312 - accuracy: 0.2781 - val_loss: -312821.0312 - val_accuracy: 0.3158\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -313062.3750 - accuracy: 0.2781 - val_loss: -313171.6875 - val_accuracy: 0.3158\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: -313418.5000 - accuracy: 0.2781 - val_loss: -313522.6875 - val_accuracy: 0.3158\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: -313774.8750 - accuracy: 0.2781 - val_loss: -313873.8750 - val_accuracy: 0.3158\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: -314131.5312 - accuracy: 0.2781 - val_loss: -314225.3125 - val_accuracy: 0.3158\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -314488.4688 - accuracy: 0.2781 - val_loss: -314577.0625 - val_accuracy: 0.3158\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: -314845.6562 - accuracy: 0.2781 - val_loss: -314929.0312 - val_accuracy: 0.3158\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -315203.1250 - accuracy: 0.2781 - val_loss: -315281.3125 - val_accuracy: 0.3158\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -315560.8438 - accuracy: 0.2781 - val_loss: -315633.7812 - val_accuracy: 0.3158\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -315918.8125 - accuracy: 0.2781 - val_loss: -315986.5312 - val_accuracy: 0.3158\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -316277.0938 - accuracy: 0.2781 - val_loss: -316339.5312 - val_accuracy: 0.3158\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -316635.5938 - accuracy: 0.2781 - val_loss: -316692.7500 - val_accuracy: 0.3158\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -316994.3750 - accuracy: 0.2781 - val_loss: -317046.2188 - val_accuracy: 0.3158\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: -317353.4375 - accuracy: 0.2781 - val_loss: -317400.0312 - val_accuracy: 0.3158\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: -317712.7812 - accuracy: 0.2781 - val_loss: -317754.1562 - val_accuracy: 0.3158\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -318072.3125 - accuracy: 0.2781 - val_loss: -318108.5938 - val_accuracy: 0.3158\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -318432.1875 - accuracy: 0.2781 - val_loss: -318463.2188 - val_accuracy: 0.3158\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: -318792.2812 - accuracy: 0.2781 - val_loss: -318818.0938 - val_accuracy: 0.3158\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: -319152.6875 - accuracy: 0.2781 - val_loss: -319173.2188 - val_accuracy: 0.3158\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: -319513.3125 - accuracy: 0.2781 - val_loss: -319528.5938 - val_accuracy: 0.3158\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: -319874.2188 - accuracy: 0.2781 - val_loss: -319884.1562 - val_accuracy: 0.3158\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -320235.4062 - accuracy: 0.2781 - val_loss: -320240.0938 - val_accuracy: 0.3158\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -320596.8438 - accuracy: 0.2781 - val_loss: -320596.2500 - val_accuracy: 0.3158\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -320958.5938 - accuracy: 0.2781 - val_loss: -320952.6875 - val_accuracy: 0.3158\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 0s 91ms/step - loss: -321320.5312 - accuracy: 0.2781 - val_loss: -321309.3438 - val_accuracy: 0.3158\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -321682.7812 - accuracy: 0.2781 - val_loss: -321666.2500 - val_accuracy: 0.3158\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -322045.2812 - accuracy: 0.2781 - val_loss: -322023.4062 - val_accuracy: 0.3158\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: -322408.0625 - accuracy: 0.2781 - val_loss: -322380.8438 - val_accuracy: 0.3158\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: -322771.1250 - accuracy: 0.2781 - val_loss: -322738.5312 - val_accuracy: 0.3158\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: -323134.4375 - accuracy: 0.2781 - val_loss: -323096.4688 - val_accuracy: 0.3158\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -323498.0312 - accuracy: 0.2781 - val_loss: -323454.6562 - val_accuracy: 0.3158\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 0s 99ms/step - loss: -323861.8125 - accuracy: 0.2781 - val_loss: -323813.1250 - val_accuracy: 0.3158\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -324225.9688 - accuracy: 0.2781 - val_loss: -324171.7812 - val_accuracy: 0.3158\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -324590.3438 - accuracy: 0.2781 - val_loss: -324530.8438 - val_accuracy: 0.3158\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -324954.9375 - accuracy: 0.2781 - val_loss: -324890.1562 - val_accuracy: 0.3158\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -325319.8438 - accuracy: 0.2781 - val_loss: -325249.6875 - val_accuracy: 0.3158\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -325685.0312 - accuracy: 0.2781 - val_loss: -325609.4688 - val_accuracy: 0.3158\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -326050.4375 - accuracy: 0.2781 - val_loss: -325969.4688 - val_accuracy: 0.3158\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -326416.1875 - accuracy: 0.2781 - val_loss: -326329.7188 - val_accuracy: 0.3158\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -326782.1250 - accuracy: 0.2781 - val_loss: -326690.2188 - val_accuracy: 0.3158\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -327148.3438 - accuracy: 0.2781 - val_loss: -327050.9062 - val_accuracy: 0.3158\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 0s 90ms/step - loss: -327514.8750 - accuracy: 0.2781 - val_loss: -327412.0312 - val_accuracy: 0.3158\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -327881.6562 - accuracy: 0.2781 - val_loss: -327773.3750 - val_accuracy: 0.3158\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: -328248.6562 - accuracy: 0.2781 - val_loss: -328134.9375 - val_accuracy: 0.3158\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: -328615.9688 - accuracy: 0.2781 - val_loss: -328496.7500 - val_accuracy: 0.3158\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: -328983.5625 - accuracy: 0.2781 - val_loss: -328858.7812 - val_accuracy: 0.3158\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -329351.3750 - accuracy: 0.2781 - val_loss: -329221.0625 - val_accuracy: 0.3158\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: -329719.5000 - accuracy: 0.2781 - val_loss: -329583.7188 - val_accuracy: 0.3158\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -330087.8438 - accuracy: 0.2781 - val_loss: -329946.6562 - val_accuracy: 0.3158\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -330456.5000 - accuracy: 0.2781 - val_loss: -330309.9062 - val_accuracy: 0.3158\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -330825.5000 - accuracy: 0.2781 - val_loss: -330673.4062 - val_accuracy: 0.3158\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -331194.6875 - accuracy: 0.2781 - val_loss: -331037.2188 - val_accuracy: 0.3158\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -331564.2188 - accuracy: 0.2781 - val_loss: -331401.3750 - val_accuracy: 0.3158\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -331934.0000 - accuracy: 0.2781 - val_loss: -331765.7812 - val_accuracy: 0.3158\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -332304.0938 - accuracy: 0.2781 - val_loss: -332130.4375 - val_accuracy: 0.3158\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -332674.4375 - accuracy: 0.2781 - val_loss: -332495.3750 - val_accuracy: 0.3158\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -333045.0625 - accuracy: 0.2781 - val_loss: -332860.5000 - val_accuracy: 0.3158\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: -333415.9375 - accuracy: 0.2781 - val_loss: -333225.9062 - val_accuracy: 0.3158\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: -333787.0938 - accuracy: 0.2781 - val_loss: -333591.5625 - val_accuracy: 0.3158\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -334158.5938 - accuracy: 0.2781 - val_loss: -333957.4062 - val_accuracy: 0.3158\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: -334530.3125 - accuracy: 0.2781 - val_loss: -334323.5938 - val_accuracy: 0.3158\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -334902.3438 - accuracy: 0.2781 - val_loss: -334690.1250 - val_accuracy: 0.3158\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -335274.6875 - accuracy: 0.2781 - val_loss: -335056.9375 - val_accuracy: 0.3158\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -335647.3125 - accuracy: 0.2781 - val_loss: -335424.0000 - val_accuracy: 0.3158\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -336020.1562 - accuracy: 0.2781 - val_loss: -335791.2500 - val_accuracy: 0.3158\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -336393.3125 - accuracy: 0.2781 - val_loss: -336158.8125 - val_accuracy: 0.3158\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -336766.8750 - accuracy: 0.2781 - val_loss: -336526.4688 - val_accuracy: 0.3158\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -337140.8750 - accuracy: 0.2781 - val_loss: -336894.4688 - val_accuracy: 0.3158\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -337515.1875 - accuracy: 0.2781 - val_loss: -337262.5938 - val_accuracy: 0.3158\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -337889.8750 - accuracy: 0.2781 - val_loss: -337631.0000 - val_accuracy: 0.3158\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -338264.8125 - accuracy: 0.2781 - val_loss: -337999.6250 - val_accuracy: 0.3158\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -338640.1250 - accuracy: 0.2781 - val_loss: -338368.5312 - val_accuracy: 0.3158\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -339015.7188 - accuracy: 0.2781 - val_loss: -338737.7500 - val_accuracy: 0.3158\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: -339391.6250 - accuracy: 0.2781 - val_loss: -339107.1562 - val_accuracy: 0.3158\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -339767.8438 - accuracy: 0.2781 - val_loss: -339476.7812 - val_accuracy: 0.3158\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -340144.3438 - accuracy: 0.2781 - val_loss: -339846.6250 - val_accuracy: 0.3158\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -340521.1562 - accuracy: 0.2781 - val_loss: -340216.7500 - val_accuracy: 0.3158\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -340898.2812 - accuracy: 0.2781 - val_loss: -340587.0938 - val_accuracy: 0.3158\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -341275.6250 - accuracy: 0.2781 - val_loss: -340957.6875 - val_accuracy: 0.3158\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -341653.3125 - accuracy: 0.2781 - val_loss: -341328.5312 - val_accuracy: 0.3158\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -342031.2500 - accuracy: 0.2781 - val_loss: -341699.8438 - val_accuracy: 0.3158\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -342409.5000 - accuracy: 0.2781 - val_loss: -342071.3438 - val_accuracy: 0.3158\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -342788.0625 - accuracy: 0.2781 - val_loss: -342443.0000 - val_accuracy: 0.3158\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -343166.8125 - accuracy: 0.2781 - val_loss: -342815.0000 - val_accuracy: 0.3158\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: -343545.9062 - accuracy: 0.2781 - val_loss: -343187.1562 - val_accuracy: 0.3158\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -343925.2812 - accuracy: 0.2781 - val_loss: -343559.5938 - val_accuracy: 0.3158\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -344304.9375 - accuracy: 0.2781 - val_loss: -343932.2812 - val_accuracy: 0.3158\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -344684.8438 - accuracy: 0.2781 - val_loss: -344305.1562 - val_accuracy: 0.3158\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -345065.0312 - accuracy: 0.2781 - val_loss: -344678.2812 - val_accuracy: 0.3158\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -345445.5000 - accuracy: 0.2781 - val_loss: -345051.6875 - val_accuracy: 0.3158\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -345826.2188 - accuracy: 0.2781 - val_loss: -345425.3125 - val_accuracy: 0.3158\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: -346207.2500 - accuracy: 0.2781 - val_loss: -345799.2500 - val_accuracy: 0.3158\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -346588.5312 - accuracy: 0.2781 - val_loss: -346173.5938 - val_accuracy: 0.3158\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -346970.0938 - accuracy: 0.2781 - val_loss: -346548.1875 - val_accuracy: 0.3158\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -347351.9062 - accuracy: 0.2781 - val_loss: -346923.1250 - val_accuracy: 0.3158\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -347734.0312 - accuracy: 0.2781 - val_loss: -347298.3125 - val_accuracy: 0.3158\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -348116.4375 - accuracy: 0.2781 - val_loss: -347673.8438 - val_accuracy: 0.3158\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -348499.0625 - accuracy: 0.2781 - val_loss: -348049.5312 - val_accuracy: 0.3158\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -348881.9688 - accuracy: 0.2781 - val_loss: -348425.4375 - val_accuracy: 0.3158\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -349265.1562 - accuracy: 0.2781 - val_loss: -348801.6562 - val_accuracy: 0.3158\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -349648.5938 - accuracy: 0.2781 - val_loss: -349178.0938 - val_accuracy: 0.3158\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: -350032.3438 - accuracy: 0.2781 - val_loss: -349554.7500 - val_accuracy: 0.3158\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: -350416.3438 - accuracy: 0.2781 - val_loss: -349931.6875 - val_accuracy: 0.3158\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -350800.5938 - accuracy: 0.2781 - val_loss: -350308.8750 - val_accuracy: 0.3158\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -351185.1250 - accuracy: 0.2781 - val_loss: -350686.2812 - val_accuracy: 0.3158\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -351569.9375 - accuracy: 0.2781 - val_loss: -351064.0938 - val_accuracy: 0.3158\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -351955.0625 - accuracy: 0.2781 - val_loss: -351442.1562 - val_accuracy: 0.3158\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -352340.3438 - accuracy: 0.2781 - val_loss: -351820.4062 - val_accuracy: 0.3158\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -352726.0000 - accuracy: 0.2781 - val_loss: -352198.9375 - val_accuracy: 0.3158\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -353111.9062 - accuracy: 0.2781 - val_loss: -352577.6875 - val_accuracy: 0.3158\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -353498.0625 - accuracy: 0.2781 - val_loss: -352956.6562 - val_accuracy: 0.3158\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -353884.4688 - accuracy: 0.2781 - val_loss: -353335.9062 - val_accuracy: 0.3158\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -354271.1875 - accuracy: 0.2781 - val_loss: -353715.5000 - val_accuracy: 0.3158\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: -354658.1875 - accuracy: 0.2781 - val_loss: -354095.3125 - val_accuracy: 0.3158\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -355045.4375 - accuracy: 0.2781 - val_loss: -354475.5312 - val_accuracy: 0.3158\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: -355433.0000 - accuracy: 0.2781 - val_loss: -354855.8750 - val_accuracy: 0.3158\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -355820.7500 - accuracy: 0.2781 - val_loss: -355236.5312 - val_accuracy: 0.3158\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -356208.8438 - accuracy: 0.2781 - val_loss: -355617.4062 - val_accuracy: 0.3158\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -356597.1562 - accuracy: 0.2781 - val_loss: -355998.4688 - val_accuracy: 0.3158\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -356985.8125 - accuracy: 0.2781 - val_loss: -356379.8438 - val_accuracy: 0.3158\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -357374.7188 - accuracy: 0.2781 - val_loss: -356761.4688 - val_accuracy: 0.3158\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -357763.9062 - accuracy: 0.2781 - val_loss: -357143.3750 - val_accuracy: 0.3158\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -358153.3438 - accuracy: 0.2781 - val_loss: -357525.5938 - val_accuracy: 0.3158\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -358543.0625 - accuracy: 0.2781 - val_loss: -357908.0938 - val_accuracy: 0.3158\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -358933.0938 - accuracy: 0.2781 - val_loss: -358290.8438 - val_accuracy: 0.3158\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -359323.3750 - accuracy: 0.2781 - val_loss: -358673.8125 - val_accuracy: 0.3158\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -359713.9062 - accuracy: 0.2781 - val_loss: -359057.0625 - val_accuracy: 0.3158\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: -360104.6875 - accuracy: 0.2781 - val_loss: -359440.5000 - val_accuracy: 0.3158\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -360495.8125 - accuracy: 0.2781 - val_loss: -359824.1875 - val_accuracy: 0.3158\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -360887.1562 - accuracy: 0.2781 - val_loss: -360208.2188 - val_accuracy: 0.3158\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: -361278.7812 - accuracy: 0.2781 - val_loss: -360592.4688 - val_accuracy: 0.3158\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -361670.6875 - accuracy: 0.2781 - val_loss: -360977.0312 - val_accuracy: 0.3158\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -362062.8750 - accuracy: 0.2781 - val_loss: -361361.8125 - val_accuracy: 0.3158\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -362455.2812 - accuracy: 0.2781 - val_loss: -361746.7812 - val_accuracy: 0.3158\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: -362848.0000 - accuracy: 0.2781 - val_loss: -362132.0938 - val_accuracy: 0.3158\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -363241.0000 - accuracy: 0.2781 - val_loss: -362517.6562 - val_accuracy: 0.3158\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -363634.2500 - accuracy: 0.2781 - val_loss: -362903.4688 - val_accuracy: 0.3158\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -364027.7500 - accuracy: 0.2781 - val_loss: -363289.6562 - val_accuracy: 0.3158\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -364421.5625 - accuracy: 0.2781 - val_loss: -363676.0625 - val_accuracy: 0.3158\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -364815.6562 - accuracy: 0.2781 - val_loss: -364062.6875 - val_accuracy: 0.3158\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -365210.0000 - accuracy: 0.2781 - val_loss: -364449.5938 - val_accuracy: 0.3158\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -365604.5938 - accuracy: 0.2781 - val_loss: -364836.6250 - val_accuracy: 0.3158\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -365999.4688 - accuracy: 0.2781 - val_loss: -365223.9688 - val_accuracy: 0.3158\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -366394.5938 - accuracy: 0.2781 - val_loss: -365611.5938 - val_accuracy: 0.3158\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -366790.0938 - accuracy: 0.2781 - val_loss: -365999.4688 - val_accuracy: 0.3158\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -367185.7500 - accuracy: 0.2781 - val_loss: -366387.6250 - val_accuracy: 0.3158\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -367581.7188 - accuracy: 0.2781 - val_loss: -366776.0938 - val_accuracy: 0.3158\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -367977.9375 - accuracy: 0.2781 - val_loss: -367164.8125 - val_accuracy: 0.3158\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -368374.4375 - accuracy: 0.2781 - val_loss: -367553.8438 - val_accuracy: 0.3158\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -368771.2812 - accuracy: 0.2781 - val_loss: -367943.0625 - val_accuracy: 0.3158\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -369168.3125 - accuracy: 0.2781 - val_loss: -368332.5312 - val_accuracy: 0.3158\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -369565.6250 - accuracy: 0.2781 - val_loss: -368722.1875 - val_accuracy: 0.3158\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -369963.2500 - accuracy: 0.2781 - val_loss: -369112.0938 - val_accuracy: 0.3158\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -370361.1250 - accuracy: 0.2781 - val_loss: -369502.3750 - val_accuracy: 0.3158\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -370759.2500 - accuracy: 0.2781 - val_loss: -369892.8750 - val_accuracy: 0.3158\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -371157.6875 - accuracy: 0.2781 - val_loss: -370283.6562 - val_accuracy: 0.3158\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: -371556.4062 - accuracy: 0.2781 - val_loss: -370674.6875 - val_accuracy: 0.3158\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -371955.3750 - accuracy: 0.2781 - val_loss: -371066.0000 - val_accuracy: 0.3158\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -372354.6562 - accuracy: 0.2781 - val_loss: -371457.5625 - val_accuracy: 0.3158\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -372754.1562 - accuracy: 0.2781 - val_loss: -371849.4375 - val_accuracy: 0.3158\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -373154.0000 - accuracy: 0.2781 - val_loss: -372241.5938 - val_accuracy: 0.3158\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -373554.0625 - accuracy: 0.2781 - val_loss: -372633.9375 - val_accuracy: 0.3158\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -373954.4062 - accuracy: 0.2781 - val_loss: -373026.5625 - val_accuracy: 0.3158\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: -374355.0312 - accuracy: 0.2781 - val_loss: -373419.3750 - val_accuracy: 0.3158\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -374755.9375 - accuracy: 0.2781 - val_loss: -373812.4062 - val_accuracy: 0.3158\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -375157.1562 - accuracy: 0.2781 - val_loss: -374205.7500 - val_accuracy: 0.3158\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -375558.6875 - accuracy: 0.2781 - val_loss: -374599.3125 - val_accuracy: 0.3158\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -375960.3750 - accuracy: 0.2781 - val_loss: -374993.2188 - val_accuracy: 0.3158\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -376362.4062 - accuracy: 0.2781 - val_loss: -375387.4062 - val_accuracy: 0.3158\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -376764.7188 - accuracy: 0.2781 - val_loss: -375782.0000 - val_accuracy: 0.3158\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: -377167.3125 - accuracy: 0.2781 - val_loss: -376176.8750 - val_accuracy: 0.3158\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -377570.1562 - accuracy: 0.2781 - val_loss: -376571.9375 - val_accuracy: 0.3158\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -377973.2812 - accuracy: 0.2781 - val_loss: -376967.2188 - val_accuracy: 0.3158\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: -378376.6875 - accuracy: 0.2781 - val_loss: -377362.7812 - val_accuracy: 0.3158\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -378780.4062 - accuracy: 0.2781 - val_loss: -377758.5625 - val_accuracy: 0.3158\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: -379184.3438 - accuracy: 0.2781 - val_loss: -378154.5625 - val_accuracy: 0.3158\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: -379588.5625 - accuracy: 0.2781 - val_loss: -378550.7500 - val_accuracy: 0.3158\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -379993.1250 - accuracy: 0.2781 - val_loss: -378947.2500 - val_accuracy: 0.3158\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -380397.9062 - accuracy: 0.2781 - val_loss: -379344.0938 - val_accuracy: 0.3158\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -380802.9688 - accuracy: 0.2781 - val_loss: -379741.2500 - val_accuracy: 0.3158\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -381208.3438 - accuracy: 0.2781 - val_loss: -380138.6562 - val_accuracy: 0.3158\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: -381614.0000 - accuracy: 0.2781 - val_loss: -380536.3125 - val_accuracy: 0.3158\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -382019.8750 - accuracy: 0.2781 - val_loss: -380934.1875 - val_accuracy: 0.3158\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: -382426.0938 - accuracy: 0.2781 - val_loss: -381332.2812 - val_accuracy: 0.3158\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -382832.5625 - accuracy: 0.2781 - val_loss: -381730.7500 - val_accuracy: 0.3158\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -383239.2812 - accuracy: 0.2781 - val_loss: -382129.5625 - val_accuracy: 0.3158\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -383646.2812 - accuracy: 0.2781 - val_loss: -382528.5625 - val_accuracy: 0.3158\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -384053.5938 - accuracy: 0.2781 - val_loss: -382927.8125 - val_accuracy: 0.3158\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -384461.1875 - accuracy: 0.2781 - val_loss: -383327.2500 - val_accuracy: 0.3158\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -384869.0000 - accuracy: 0.2781 - val_loss: -383726.9688 - val_accuracy: 0.3158\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -385277.1250 - accuracy: 0.2781 - val_loss: -384126.9062 - val_accuracy: 0.3158\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -385685.5312 - accuracy: 0.2781 - val_loss: -384527.1875 - val_accuracy: 0.3158\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: -386094.1875 - accuracy: 0.2781 - val_loss: -384927.7500 - val_accuracy: 0.3158\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -386503.0938 - accuracy: 0.2781 - val_loss: -385328.5000 - val_accuracy: 0.3158\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -386912.3438 - accuracy: 0.2781 - val_loss: -385729.5938 - val_accuracy: 0.3158\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -387321.8125 - accuracy: 0.2781 - val_loss: -386130.9062 - val_accuracy: 0.3158\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -387731.5938 - accuracy: 0.2781 - val_loss: -386532.4688 - val_accuracy: 0.3158\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -388141.6250 - accuracy: 0.2781 - val_loss: -386934.2812 - val_accuracy: 0.3158\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -388551.9375 - accuracy: 0.2781 - val_loss: -387336.4062 - val_accuracy: 0.3158\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: -388962.5000 - accuracy: 0.2781 - val_loss: -387738.7500 - val_accuracy: 0.3158\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -389373.3438 - accuracy: 0.2781 - val_loss: -388141.3125 - val_accuracy: 0.3158\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: -389784.4688 - accuracy: 0.2781 - val_loss: -388544.1875 - val_accuracy: 0.3158\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 0s 94ms/step - loss: -390195.9062 - accuracy: 0.2781 - val_loss: -388947.3438 - val_accuracy: 0.3158\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: -390607.5312 - accuracy: 0.2781 - val_loss: -389350.7812 - val_accuracy: 0.3158\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -391019.5000 - accuracy: 0.2781 - val_loss: -389754.4375 - val_accuracy: 0.3158\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: -391431.7500 - accuracy: 0.2781 - val_loss: -390158.3438 - val_accuracy: 0.3158\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 0s 93ms/step - loss: -391844.2188 - accuracy: 0.2781 - val_loss: -390562.4375 - val_accuracy: 0.3158\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -392257.0000 - accuracy: 0.2781 - val_loss: -390966.9062 - val_accuracy: 0.3158\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -392670.0312 - accuracy: 0.2781 - val_loss: -391371.5938 - val_accuracy: 0.3158\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: -393083.3750 - accuracy: 0.2781 - val_loss: -391776.5625 - val_accuracy: 0.3158\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 0s 92ms/step - loss: -393496.9375 - accuracy: 0.2781 - val_loss: -392181.7812 - val_accuracy: 0.3158\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -393910.8125 - accuracy: 0.2781 - val_loss: -392587.2812 - val_accuracy: 0.3158\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: -394324.9375 - accuracy: 0.2781 - val_loss: -392993.0000 - val_accuracy: 0.3158\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -394739.3750 - accuracy: 0.2781 - val_loss: -393398.9062 - val_accuracy: 0.3158\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 0s 92ms/step - loss: -395154.0625 - accuracy: 0.2781 - val_loss: -393805.0938 - val_accuracy: 0.3158\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -395569.0312 - accuracy: 0.2781 - val_loss: -394211.7500 - val_accuracy: 0.3158\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: -395984.2500 - accuracy: 0.2781 - val_loss: -394618.6250 - val_accuracy: 0.3158\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -396399.7812 - accuracy: 0.2781 - val_loss: -395025.7500 - val_accuracy: 0.3158\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -396815.5625 - accuracy: 0.2781 - val_loss: -395433.0625 - val_accuracy: 0.3158\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -397231.6250 - accuracy: 0.2781 - val_loss: -395840.6250 - val_accuracy: 0.3158\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -397648.0000 - accuracy: 0.2781 - val_loss: -396248.4375 - val_accuracy: 0.3158\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: -398064.6562 - accuracy: 0.2781 - val_loss: -396656.5312 - val_accuracy: 0.3158\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 0s 90ms/step - loss: -398481.6250 - accuracy: 0.2781 - val_loss: -397064.9062 - val_accuracy: 0.3158\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: -398898.8438 - accuracy: 0.2781 - val_loss: -397473.5000 - val_accuracy: 0.3158\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: -399316.3125 - accuracy: 0.2781 - val_loss: -397882.3438 - val_accuracy: 0.3158\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: -399734.0938 - accuracy: 0.2781 - val_loss: -398291.4375 - val_accuracy: 0.3158\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -400152.1562 - accuracy: 0.2781 - val_loss: -398700.8750 - val_accuracy: 0.3158\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 0s 93ms/step - loss: -400570.5000 - accuracy: 0.2781 - val_loss: -399110.6250 - val_accuracy: 0.3158\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -400989.1250 - accuracy: 0.2781 - val_loss: -399520.6875 - val_accuracy: 0.3158\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -401408.0000 - accuracy: 0.2781 - val_loss: -399931.0000 - val_accuracy: 0.3158\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: -401827.1250 - accuracy: 0.2781 - val_loss: -400341.5625 - val_accuracy: 0.3158\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: -402246.5938 - accuracy: 0.2781 - val_loss: -400752.3125 - val_accuracy: 0.3158\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -402666.2812 - accuracy: 0.2781 - val_loss: -401163.3750 - val_accuracy: 0.3158\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -403086.2812 - accuracy: 0.2781 - val_loss: -401574.5938 - val_accuracy: 0.3158\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: -403506.5625 - accuracy: 0.2781 - val_loss: -401986.1250 - val_accuracy: 0.3158\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 0s 91ms/step - loss: -403927.0938 - accuracy: 0.2781 - val_loss: -402398.0625 - val_accuracy: 0.3158\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: -404347.9375 - accuracy: 0.2781 - val_loss: -402810.1875 - val_accuracy: 0.3158\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 0s 96ms/step - loss: -404769.0312 - accuracy: 0.2781 - val_loss: -403222.5938 - val_accuracy: 0.3158\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -405190.3750 - accuracy: 0.2781 - val_loss: -403635.1875 - val_accuracy: 0.3158\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: -405612.0312 - accuracy: 0.2781 - val_loss: -404048.0938 - val_accuracy: 0.3158\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 0s 94ms/step - loss: -406034.0000 - accuracy: 0.2781 - val_loss: -404461.2500 - val_accuracy: 0.3158\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -406456.1875 - accuracy: 0.2781 - val_loss: -404874.5938 - val_accuracy: 0.3158\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: -406878.6250 - accuracy: 0.2781 - val_loss: -405288.2812 - val_accuracy: 0.3158\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79bba5d13df0>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_log = model.predict(x_test_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr8Qv-TmMHzt",
        "outputId": "0ece6e7e-9613-443a-ee1d-7b990e673bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y_pred = np.where(y_log>0.5,1,0)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb8ujxk_MU2J",
        "outputId": "d0c81ed7-c353-4dd9-fc50-682b7c03549f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_encoded = label_encoder.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "JQ8k_g85NCb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "report = classification_report(y_test_encoded, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpELPuRsMYh1",
        "outputId": "4f6ddb2b-196a-46a6-ec9a-f9010a7b9c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         6\n",
            "           1       0.27      1.00      0.42        17\n",
            "           2       0.00      0.00      0.00        10\n",
            "           3       0.00      0.00      0.00         6\n",
            "           4       0.00      0.00      0.00        11\n",
            "           5       0.00      0.00      0.00        13\n",
            "\n",
            "    accuracy                           0.27        63\n",
            "   macro avg       0.04      0.17      0.07        63\n",
            "weighted avg       0.07      0.27      0.11        63\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nyoba **SVM**"
      ],
      "metadata": {
        "id": "KmXIAU_xiCVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "P0EACxMmiFqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_random_state = 1000  # Ganti dengan nilai maksimum yang sesuai\n",
        "target_accuracy = 0.93\n",
        "best_accuracy = 0.0\n",
        "best_random_state = 0"
      ],
      "metadata": {
        "id": "Zie0MVkXkGdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df_data.drop(columns = 'Diagnosa', axis = 1)\n",
        "y = df_data['Diagnosa']\n",
        "\n",
        "for i in range(1, max_random_state + 1):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=i,stratify=y, test_size=0.2)\n",
        "  sc = StandardScaler()\n",
        "  X_train = sc.fit_transform(x_train)\n",
        "  X_test = sc.transform(x_test)\n",
        "  model = SVC(kernel='rbf', random_state=60)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  accuracy=accuracy_score(y_test, y_pred)\n",
        "  print(f\"Random State: {i}, Accuracy: {accuracy*100}\")\n",
        "  if accuracy >= target_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_random_state = i\n",
        "        print(f\"Akurasi mencapai target ({target_accuracy}) dengan random state {i}\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXr1zh_2kM7Y",
        "outputId": "0439cc95-c34f-48a4-f424-8e6e9a94d5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random State: 1, Accuracy: 86.27450980392157\n",
            "Random State: 2, Accuracy: 76.47058823529412\n",
            "Random State: 3, Accuracy: 82.35294117647058\n",
            "Random State: 4, Accuracy: 78.43137254901961\n",
            "Random State: 5, Accuracy: 84.31372549019608\n",
            "Random State: 6, Accuracy: 80.3921568627451\n",
            "Random State: 7, Accuracy: 90.19607843137256\n",
            "Random State: 8, Accuracy: 84.31372549019608\n",
            "Random State: 9, Accuracy: 82.35294117647058\n",
            "Random State: 10, Accuracy: 72.54901960784314\n",
            "Random State: 11, Accuracy: 80.3921568627451\n",
            "Random State: 12, Accuracy: 78.43137254901961\n",
            "Random State: 13, Accuracy: 86.27450980392157\n",
            "Random State: 14, Accuracy: 90.19607843137256\n",
            "Random State: 15, Accuracy: 82.35294117647058\n",
            "Random State: 16, Accuracy: 76.47058823529412\n",
            "Random State: 17, Accuracy: 76.47058823529412\n",
            "Random State: 18, Accuracy: 78.43137254901961\n",
            "Random State: 19, Accuracy: 88.23529411764706\n",
            "Random State: 20, Accuracy: 86.27450980392157\n",
            "Random State: 21, Accuracy: 84.31372549019608\n",
            "Random State: 22, Accuracy: 86.27450980392157\n",
            "Random State: 23, Accuracy: 82.35294117647058\n",
            "Random State: 24, Accuracy: 84.31372549019608\n",
            "Random State: 25, Accuracy: 84.31372549019608\n",
            "Random State: 26, Accuracy: 84.31372549019608\n",
            "Random State: 27, Accuracy: 76.47058823529412\n",
            "Random State: 28, Accuracy: 82.35294117647058\n",
            "Random State: 29, Accuracy: 74.50980392156863\n",
            "Random State: 30, Accuracy: 82.35294117647058\n",
            "Random State: 31, Accuracy: 78.43137254901961\n",
            "Random State: 32, Accuracy: 86.27450980392157\n",
            "Random State: 33, Accuracy: 84.31372549019608\n",
            "Random State: 34, Accuracy: 82.35294117647058\n",
            "Random State: 35, Accuracy: 80.3921568627451\n",
            "Random State: 36, Accuracy: 80.3921568627451\n",
            "Random State: 37, Accuracy: 88.23529411764706\n",
            "Random State: 38, Accuracy: 88.23529411764706\n",
            "Random State: 39, Accuracy: 82.35294117647058\n",
            "Random State: 40, Accuracy: 82.35294117647058\n",
            "Random State: 41, Accuracy: 80.3921568627451\n",
            "Random State: 42, Accuracy: 86.27450980392157\n",
            "Random State: 43, Accuracy: 82.35294117647058\n",
            "Random State: 44, Accuracy: 76.47058823529412\n",
            "Random State: 45, Accuracy: 80.3921568627451\n",
            "Random State: 46, Accuracy: 80.3921568627451\n",
            "Random State: 47, Accuracy: 82.35294117647058\n",
            "Random State: 48, Accuracy: 74.50980392156863\n",
            "Random State: 49, Accuracy: 84.31372549019608\n",
            "Random State: 50, Accuracy: 80.3921568627451\n",
            "Random State: 51, Accuracy: 78.43137254901961\n",
            "Random State: 52, Accuracy: 82.35294117647058\n",
            "Random State: 53, Accuracy: 80.3921568627451\n",
            "Random State: 54, Accuracy: 72.54901960784314\n",
            "Random State: 55, Accuracy: 88.23529411764706\n",
            "Random State: 56, Accuracy: 78.43137254901961\n",
            "Random State: 57, Accuracy: 78.43137254901961\n",
            "Random State: 58, Accuracy: 80.3921568627451\n",
            "Random State: 59, Accuracy: 84.31372549019608\n",
            "Random State: 60, Accuracy: 82.35294117647058\n",
            "Random State: 61, Accuracy: 76.47058823529412\n",
            "Random State: 62, Accuracy: 82.35294117647058\n",
            "Random State: 63, Accuracy: 92.15686274509804\n",
            "Random State: 64, Accuracy: 86.27450980392157\n",
            "Random State: 65, Accuracy: 82.35294117647058\n",
            "Random State: 66, Accuracy: 82.35294117647058\n",
            "Random State: 67, Accuracy: 90.19607843137256\n",
            "Random State: 68, Accuracy: 80.3921568627451\n",
            "Random State: 69, Accuracy: 78.43137254901961\n",
            "Random State: 70, Accuracy: 86.27450980392157\n",
            "Random State: 71, Accuracy: 80.3921568627451\n",
            "Random State: 72, Accuracy: 76.47058823529412\n",
            "Random State: 73, Accuracy: 82.35294117647058\n",
            "Random State: 74, Accuracy: 86.27450980392157\n",
            "Random State: 75, Accuracy: 76.47058823529412\n",
            "Random State: 76, Accuracy: 78.43137254901961\n",
            "Random State: 77, Accuracy: 80.3921568627451\n",
            "Random State: 78, Accuracy: 84.31372549019608\n",
            "Random State: 79, Accuracy: 76.47058823529412\n",
            "Random State: 80, Accuracy: 88.23529411764706\n",
            "Random State: 81, Accuracy: 78.43137254901961\n",
            "Random State: 82, Accuracy: 78.43137254901961\n",
            "Random State: 83, Accuracy: 84.31372549019608\n",
            "Random State: 84, Accuracy: 78.43137254901961\n",
            "Random State: 85, Accuracy: 84.31372549019608\n",
            "Random State: 86, Accuracy: 78.43137254901961\n",
            "Random State: 87, Accuracy: 84.31372549019608\n",
            "Random State: 88, Accuracy: 74.50980392156863\n",
            "Random State: 89, Accuracy: 84.31372549019608\n",
            "Random State: 90, Accuracy: 84.31372549019608\n",
            "Random State: 91, Accuracy: 82.35294117647058\n",
            "Random State: 92, Accuracy: 84.31372549019608\n",
            "Random State: 93, Accuracy: 84.31372549019608\n",
            "Random State: 94, Accuracy: 86.27450980392157\n",
            "Random State: 95, Accuracy: 88.23529411764706\n",
            "Random State: 96, Accuracy: 78.43137254901961\n",
            "Random State: 97, Accuracy: 88.23529411764706\n",
            "Random State: 98, Accuracy: 84.31372549019608\n",
            "Random State: 99, Accuracy: 76.47058823529412\n",
            "Random State: 100, Accuracy: 86.27450980392157\n",
            "Random State: 101, Accuracy: 80.3921568627451\n",
            "Random State: 102, Accuracy: 76.47058823529412\n",
            "Random State: 103, Accuracy: 78.43137254901961\n",
            "Random State: 104, Accuracy: 78.43137254901961\n",
            "Random State: 105, Accuracy: 80.3921568627451\n",
            "Random State: 106, Accuracy: 88.23529411764706\n",
            "Random State: 107, Accuracy: 90.19607843137256\n",
            "Random State: 108, Accuracy: 82.35294117647058\n",
            "Random State: 109, Accuracy: 86.27450980392157\n",
            "Random State: 110, Accuracy: 68.62745098039215\n",
            "Random State: 111, Accuracy: 86.27450980392157\n",
            "Random State: 112, Accuracy: 82.35294117647058\n",
            "Random State: 113, Accuracy: 84.31372549019608\n",
            "Random State: 114, Accuracy: 84.31372549019608\n",
            "Random State: 115, Accuracy: 74.50980392156863\n",
            "Random State: 116, Accuracy: 82.35294117647058\n",
            "Random State: 117, Accuracy: 82.35294117647058\n",
            "Random State: 118, Accuracy: 78.43137254901961\n",
            "Random State: 119, Accuracy: 74.50980392156863\n",
            "Random State: 120, Accuracy: 78.43137254901961\n",
            "Random State: 121, Accuracy: 74.50980392156863\n",
            "Random State: 122, Accuracy: 82.35294117647058\n",
            "Random State: 123, Accuracy: 78.43137254901961\n",
            "Random State: 124, Accuracy: 80.3921568627451\n",
            "Random State: 125, Accuracy: 84.31372549019608\n",
            "Random State: 126, Accuracy: 82.35294117647058\n",
            "Random State: 127, Accuracy: 80.3921568627451\n",
            "Random State: 128, Accuracy: 82.35294117647058\n",
            "Random State: 129, Accuracy: 72.54901960784314\n",
            "Random State: 130, Accuracy: 72.54901960784314\n",
            "Random State: 131, Accuracy: 82.35294117647058\n",
            "Random State: 132, Accuracy: 80.3921568627451\n",
            "Random State: 133, Accuracy: 76.47058823529412\n",
            "Random State: 134, Accuracy: 86.27450980392157\n",
            "Random State: 135, Accuracy: 86.27450980392157\n",
            "Random State: 136, Accuracy: 90.19607843137256\n",
            "Random State: 137, Accuracy: 80.3921568627451\n",
            "Random State: 138, Accuracy: 80.3921568627451\n",
            "Random State: 139, Accuracy: 80.3921568627451\n",
            "Random State: 140, Accuracy: 80.3921568627451\n",
            "Random State: 141, Accuracy: 76.47058823529412\n",
            "Random State: 142, Accuracy: 80.3921568627451\n",
            "Random State: 143, Accuracy: 80.3921568627451\n",
            "Random State: 144, Accuracy: 76.47058823529412\n",
            "Random State: 145, Accuracy: 88.23529411764706\n",
            "Random State: 146, Accuracy: 66.66666666666666\n",
            "Random State: 147, Accuracy: 80.3921568627451\n",
            "Random State: 148, Accuracy: 78.43137254901961\n",
            "Random State: 149, Accuracy: 84.31372549019608\n",
            "Random State: 150, Accuracy: 84.31372549019608\n",
            "Random State: 151, Accuracy: 70.58823529411765\n",
            "Random State: 152, Accuracy: 88.23529411764706\n",
            "Random State: 153, Accuracy: 78.43137254901961\n",
            "Random State: 154, Accuracy: 76.47058823529412\n",
            "Random State: 155, Accuracy: 80.3921568627451\n",
            "Random State: 156, Accuracy: 82.35294117647058\n",
            "Random State: 157, Accuracy: 80.3921568627451\n",
            "Random State: 158, Accuracy: 84.31372549019608\n",
            "Random State: 159, Accuracy: 86.27450980392157\n",
            "Random State: 160, Accuracy: 74.50980392156863\n",
            "Random State: 161, Accuracy: 88.23529411764706\n",
            "Random State: 162, Accuracy: 88.23529411764706\n",
            "Random State: 163, Accuracy: 76.47058823529412\n",
            "Random State: 164, Accuracy: 78.43137254901961\n",
            "Random State: 165, Accuracy: 80.3921568627451\n",
            "Random State: 166, Accuracy: 82.35294117647058\n",
            "Random State: 167, Accuracy: 82.35294117647058\n",
            "Random State: 168, Accuracy: 86.27450980392157\n",
            "Random State: 169, Accuracy: 84.31372549019608\n",
            "Random State: 170, Accuracy: 84.31372549019608\n",
            "Random State: 171, Accuracy: 86.27450980392157\n",
            "Random State: 172, Accuracy: 82.35294117647058\n",
            "Random State: 173, Accuracy: 88.23529411764706\n",
            "Random State: 174, Accuracy: 86.27450980392157\n",
            "Random State: 175, Accuracy: 74.50980392156863\n",
            "Random State: 176, Accuracy: 84.31372549019608\n",
            "Random State: 177, Accuracy: 78.43137254901961\n",
            "Random State: 178, Accuracy: 82.35294117647058\n",
            "Random State: 179, Accuracy: 82.35294117647058\n",
            "Random State: 180, Accuracy: 86.27450980392157\n",
            "Random State: 181, Accuracy: 82.35294117647058\n",
            "Random State: 182, Accuracy: 78.43137254901961\n",
            "Random State: 183, Accuracy: 86.27450980392157\n",
            "Random State: 184, Accuracy: 90.19607843137256\n",
            "Random State: 185, Accuracy: 76.47058823529412\n",
            "Random State: 186, Accuracy: 82.35294117647058\n",
            "Random State: 187, Accuracy: 84.31372549019608\n",
            "Random State: 188, Accuracy: 86.27450980392157\n",
            "Random State: 189, Accuracy: 84.31372549019608\n",
            "Random State: 190, Accuracy: 78.43137254901961\n",
            "Random State: 191, Accuracy: 80.3921568627451\n",
            "Random State: 192, Accuracy: 86.27450980392157\n",
            "Random State: 193, Accuracy: 76.47058823529412\n",
            "Random State: 194, Accuracy: 82.35294117647058\n",
            "Random State: 195, Accuracy: 90.19607843137256\n",
            "Random State: 196, Accuracy: 80.3921568627451\n",
            "Random State: 197, Accuracy: 88.23529411764706\n",
            "Random State: 198, Accuracy: 78.43137254901961\n",
            "Random State: 199, Accuracy: 88.23529411764706\n",
            "Random State: 200, Accuracy: 88.23529411764706\n",
            "Random State: 201, Accuracy: 78.43137254901961\n",
            "Random State: 202, Accuracy: 78.43137254901961\n",
            "Random State: 203, Accuracy: 84.31372549019608\n",
            "Random State: 204, Accuracy: 80.3921568627451\n",
            "Random State: 205, Accuracy: 88.23529411764706\n",
            "Random State: 206, Accuracy: 86.27450980392157\n",
            "Random State: 207, Accuracy: 82.35294117647058\n",
            "Random State: 208, Accuracy: 76.47058823529412\n",
            "Random State: 209, Accuracy: 84.31372549019608\n",
            "Random State: 210, Accuracy: 80.3921568627451\n",
            "Random State: 211, Accuracy: 90.19607843137256\n",
            "Random State: 212, Accuracy: 76.47058823529412\n",
            "Random State: 213, Accuracy: 76.47058823529412\n",
            "Random State: 214, Accuracy: 78.43137254901961\n",
            "Random State: 215, Accuracy: 84.31372549019608\n",
            "Random State: 216, Accuracy: 86.27450980392157\n",
            "Random State: 217, Accuracy: 80.3921568627451\n",
            "Random State: 218, Accuracy: 84.31372549019608\n",
            "Random State: 219, Accuracy: 80.3921568627451\n",
            "Random State: 220, Accuracy: 74.50980392156863\n",
            "Random State: 221, Accuracy: 82.35294117647058\n",
            "Random State: 222, Accuracy: 80.3921568627451\n",
            "Random State: 223, Accuracy: 68.62745098039215\n",
            "Random State: 224, Accuracy: 84.31372549019608\n",
            "Random State: 225, Accuracy: 74.50980392156863\n",
            "Random State: 226, Accuracy: 76.47058823529412\n",
            "Random State: 227, Accuracy: 80.3921568627451\n",
            "Random State: 228, Accuracy: 82.35294117647058\n",
            "Random State: 229, Accuracy: 86.27450980392157\n",
            "Random State: 230, Accuracy: 78.43137254901961\n",
            "Random State: 231, Accuracy: 80.3921568627451\n",
            "Random State: 232, Accuracy: 86.27450980392157\n",
            "Random State: 233, Accuracy: 82.35294117647058\n",
            "Random State: 234, Accuracy: 78.43137254901961\n",
            "Random State: 235, Accuracy: 86.27450980392157\n",
            "Random State: 236, Accuracy: 72.54901960784314\n",
            "Random State: 237, Accuracy: 92.15686274509804\n",
            "Random State: 238, Accuracy: 80.3921568627451\n",
            "Random State: 239, Accuracy: 80.3921568627451\n",
            "Random State: 240, Accuracy: 88.23529411764706\n",
            "Random State: 241, Accuracy: 86.27450980392157\n",
            "Random State: 242, Accuracy: 78.43137254901961\n",
            "Random State: 243, Accuracy: 76.47058823529412\n",
            "Random State: 244, Accuracy: 68.62745098039215\n",
            "Random State: 245, Accuracy: 82.35294117647058\n",
            "Random State: 246, Accuracy: 82.35294117647058\n",
            "Random State: 247, Accuracy: 84.31372549019608\n",
            "Random State: 248, Accuracy: 86.27450980392157\n",
            "Random State: 249, Accuracy: 88.23529411764706\n",
            "Random State: 250, Accuracy: 84.31372549019608\n",
            "Random State: 251, Accuracy: 84.31372549019608\n",
            "Random State: 252, Accuracy: 80.3921568627451\n",
            "Random State: 253, Accuracy: 82.35294117647058\n",
            "Random State: 254, Accuracy: 86.27450980392157\n",
            "Random State: 255, Accuracy: 86.27450980392157\n",
            "Random State: 256, Accuracy: 78.43137254901961\n",
            "Random State: 257, Accuracy: 90.19607843137256\n",
            "Random State: 258, Accuracy: 88.23529411764706\n",
            "Random State: 259, Accuracy: 84.31372549019608\n",
            "Random State: 260, Accuracy: 80.3921568627451\n",
            "Random State: 261, Accuracy: 86.27450980392157\n",
            "Random State: 262, Accuracy: 82.35294117647058\n",
            "Random State: 263, Accuracy: 84.31372549019608\n",
            "Random State: 264, Accuracy: 80.3921568627451\n",
            "Random State: 265, Accuracy: 92.15686274509804\n",
            "Random State: 266, Accuracy: 84.31372549019608\n",
            "Random State: 267, Accuracy: 80.3921568627451\n",
            "Random State: 268, Accuracy: 82.35294117647058\n",
            "Random State: 269, Accuracy: 84.31372549019608\n",
            "Random State: 270, Accuracy: 82.35294117647058\n",
            "Random State: 271, Accuracy: 70.58823529411765\n",
            "Random State: 272, Accuracy: 80.3921568627451\n",
            "Random State: 273, Accuracy: 80.3921568627451\n",
            "Random State: 274, Accuracy: 66.66666666666666\n",
            "Random State: 275, Accuracy: 78.43137254901961\n",
            "Random State: 276, Accuracy: 86.27450980392157\n",
            "Random State: 277, Accuracy: 78.43137254901961\n",
            "Random State: 278, Accuracy: 78.43137254901961\n",
            "Random State: 279, Accuracy: 80.3921568627451\n",
            "Random State: 280, Accuracy: 76.47058823529412\n",
            "Random State: 281, Accuracy: 82.35294117647058\n",
            "Random State: 282, Accuracy: 84.31372549019608\n",
            "Random State: 283, Accuracy: 84.31372549019608\n",
            "Random State: 284, Accuracy: 82.35294117647058\n",
            "Random State: 285, Accuracy: 84.31372549019608\n",
            "Random State: 286, Accuracy: 78.43137254901961\n",
            "Random State: 287, Accuracy: 84.31372549019608\n",
            "Random State: 288, Accuracy: 88.23529411764706\n",
            "Random State: 289, Accuracy: 76.47058823529412\n",
            "Random State: 290, Accuracy: 90.19607843137256\n",
            "Random State: 291, Accuracy: 86.27450980392157\n",
            "Random State: 292, Accuracy: 88.23529411764706\n",
            "Random State: 293, Accuracy: 82.35294117647058\n",
            "Random State: 294, Accuracy: 84.31372549019608\n",
            "Random State: 295, Accuracy: 80.3921568627451\n",
            "Random State: 296, Accuracy: 82.35294117647058\n",
            "Random State: 297, Accuracy: 86.27450980392157\n",
            "Random State: 298, Accuracy: 80.3921568627451\n",
            "Random State: 299, Accuracy: 88.23529411764706\n",
            "Random State: 300, Accuracy: 84.31372549019608\n",
            "Random State: 301, Accuracy: 76.47058823529412\n",
            "Random State: 302, Accuracy: 80.3921568627451\n",
            "Random State: 303, Accuracy: 78.43137254901961\n",
            "Random State: 304, Accuracy: 86.27450980392157\n",
            "Random State: 305, Accuracy: 92.15686274509804\n",
            "Random State: 306, Accuracy: 84.31372549019608\n",
            "Random State: 307, Accuracy: 82.35294117647058\n",
            "Random State: 308, Accuracy: 78.43137254901961\n",
            "Random State: 309, Accuracy: 74.50980392156863\n",
            "Random State: 310, Accuracy: 88.23529411764706\n",
            "Random State: 311, Accuracy: 80.3921568627451\n",
            "Random State: 312, Accuracy: 76.47058823529412\n",
            "Random State: 313, Accuracy: 86.27450980392157\n",
            "Random State: 314, Accuracy: 88.23529411764706\n",
            "Random State: 315, Accuracy: 86.27450980392157\n",
            "Random State: 316, Accuracy: 78.43137254901961\n",
            "Random State: 317, Accuracy: 74.50980392156863\n",
            "Random State: 318, Accuracy: 76.47058823529412\n",
            "Random State: 319, Accuracy: 78.43137254901961\n",
            "Random State: 320, Accuracy: 80.3921568627451\n",
            "Random State: 321, Accuracy: 84.31372549019608\n",
            "Random State: 322, Accuracy: 86.27450980392157\n",
            "Random State: 323, Accuracy: 92.15686274509804\n",
            "Random State: 324, Accuracy: 78.43137254901961\n",
            "Random State: 325, Accuracy: 82.35294117647058\n",
            "Random State: 326, Accuracy: 84.31372549019608\n",
            "Random State: 327, Accuracy: 84.31372549019608\n",
            "Random State: 328, Accuracy: 80.3921568627451\n",
            "Random State: 329, Accuracy: 80.3921568627451\n",
            "Random State: 330, Accuracy: 82.35294117647058\n",
            "Random State: 331, Accuracy: 82.35294117647058\n",
            "Random State: 332, Accuracy: 86.27450980392157\n",
            "Random State: 333, Accuracy: 84.31372549019608\n",
            "Random State: 334, Accuracy: 80.3921568627451\n",
            "Random State: 335, Accuracy: 78.43137254901961\n",
            "Random State: 336, Accuracy: 84.31372549019608\n",
            "Random State: 337, Accuracy: 80.3921568627451\n",
            "Random State: 338, Accuracy: 82.35294117647058\n",
            "Random State: 339, Accuracy: 84.31372549019608\n",
            "Random State: 340, Accuracy: 84.31372549019608\n",
            "Random State: 341, Accuracy: 84.31372549019608\n",
            "Random State: 342, Accuracy: 88.23529411764706\n",
            "Random State: 343, Accuracy: 78.43137254901961\n",
            "Random State: 344, Accuracy: 86.27450980392157\n",
            "Random State: 345, Accuracy: 80.3921568627451\n",
            "Random State: 346, Accuracy: 88.23529411764706\n",
            "Random State: 347, Accuracy: 70.58823529411765\n",
            "Random State: 348, Accuracy: 92.15686274509804\n",
            "Random State: 349, Accuracy: 82.35294117647058\n",
            "Random State: 350, Accuracy: 76.47058823529412\n",
            "Random State: 351, Accuracy: 80.3921568627451\n",
            "Random State: 352, Accuracy: 72.54901960784314\n",
            "Random State: 353, Accuracy: 84.31372549019608\n",
            "Random State: 354, Accuracy: 86.27450980392157\n",
            "Random State: 355, Accuracy: 92.15686274509804\n",
            "Random State: 356, Accuracy: 78.43137254901961\n",
            "Random State: 357, Accuracy: 88.23529411764706\n",
            "Random State: 358, Accuracy: 86.27450980392157\n",
            "Random State: 359, Accuracy: 84.31372549019608\n",
            "Random State: 360, Accuracy: 78.43137254901961\n",
            "Random State: 361, Accuracy: 84.31372549019608\n",
            "Random State: 362, Accuracy: 86.27450980392157\n",
            "Random State: 363, Accuracy: 90.19607843137256\n",
            "Random State: 364, Accuracy: 92.15686274509804\n",
            "Random State: 365, Accuracy: 88.23529411764706\n",
            "Random State: 366, Accuracy: 80.3921568627451\n",
            "Random State: 367, Accuracy: 84.31372549019608\n",
            "Random State: 368, Accuracy: 74.50980392156863\n",
            "Random State: 369, Accuracy: 82.35294117647058\n",
            "Random State: 370, Accuracy: 82.35294117647058\n",
            "Random State: 371, Accuracy: 84.31372549019608\n",
            "Random State: 372, Accuracy: 80.3921568627451\n",
            "Random State: 373, Accuracy: 76.47058823529412\n",
            "Random State: 374, Accuracy: 78.43137254901961\n",
            "Random State: 375, Accuracy: 88.23529411764706\n",
            "Random State: 376, Accuracy: 86.27450980392157\n",
            "Random State: 377, Accuracy: 80.3921568627451\n",
            "Random State: 378, Accuracy: 88.23529411764706\n",
            "Random State: 379, Accuracy: 88.23529411764706\n",
            "Random State: 380, Accuracy: 76.47058823529412\n",
            "Random State: 381, Accuracy: 82.35294117647058\n",
            "Random State: 382, Accuracy: 86.27450980392157\n",
            "Random State: 383, Accuracy: 84.31372549019608\n",
            "Random State: 384, Accuracy: 82.35294117647058\n",
            "Random State: 385, Accuracy: 84.31372549019608\n",
            "Random State: 386, Accuracy: 76.47058823529412\n",
            "Random State: 387, Accuracy: 90.19607843137256\n",
            "Random State: 388, Accuracy: 76.47058823529412\n",
            "Random State: 389, Accuracy: 86.27450980392157\n",
            "Random State: 390, Accuracy: 86.27450980392157\n",
            "Random State: 391, Accuracy: 90.19607843137256\n",
            "Random State: 392, Accuracy: 84.31372549019608\n",
            "Random State: 393, Accuracy: 80.3921568627451\n",
            "Random State: 394, Accuracy: 82.35294117647058\n",
            "Random State: 395, Accuracy: 88.23529411764706\n",
            "Random State: 396, Accuracy: 78.43137254901961\n",
            "Random State: 397, Accuracy: 76.47058823529412\n",
            "Random State: 398, Accuracy: 76.47058823529412\n",
            "Random State: 399, Accuracy: 82.35294117647058\n",
            "Random State: 400, Accuracy: 72.54901960784314\n",
            "Random State: 401, Accuracy: 90.19607843137256\n",
            "Random State: 402, Accuracy: 86.27450980392157\n",
            "Random State: 403, Accuracy: 78.43137254901961\n",
            "Random State: 404, Accuracy: 78.43137254901961\n",
            "Random State: 405, Accuracy: 74.50980392156863\n",
            "Random State: 406, Accuracy: 82.35294117647058\n",
            "Random State: 407, Accuracy: 86.27450980392157\n",
            "Random State: 408, Accuracy: 68.62745098039215\n",
            "Random State: 409, Accuracy: 80.3921568627451\n",
            "Random State: 410, Accuracy: 88.23529411764706\n",
            "Random State: 411, Accuracy: 84.31372549019608\n",
            "Random State: 412, Accuracy: 76.47058823529412\n",
            "Random State: 413, Accuracy: 80.3921568627451\n",
            "Random State: 414, Accuracy: 78.43137254901961\n",
            "Random State: 415, Accuracy: 86.27450980392157\n",
            "Random State: 416, Accuracy: 82.35294117647058\n",
            "Random State: 417, Accuracy: 84.31372549019608\n",
            "Random State: 418, Accuracy: 84.31372549019608\n",
            "Random State: 419, Accuracy: 88.23529411764706\n",
            "Random State: 420, Accuracy: 86.27450980392157\n",
            "Random State: 421, Accuracy: 84.31372549019608\n",
            "Random State: 422, Accuracy: 86.27450980392157\n",
            "Random State: 423, Accuracy: 86.27450980392157\n",
            "Random State: 424, Accuracy: 88.23529411764706\n",
            "Random State: 425, Accuracy: 90.19607843137256\n",
            "Random State: 426, Accuracy: 82.35294117647058\n",
            "Random State: 427, Accuracy: 84.31372549019608\n",
            "Random State: 428, Accuracy: 76.47058823529412\n",
            "Random State: 429, Accuracy: 80.3921568627451\n",
            "Random State: 430, Accuracy: 80.3921568627451\n",
            "Random State: 431, Accuracy: 86.27450980392157\n",
            "Random State: 432, Accuracy: 80.3921568627451\n",
            "Random State: 433, Accuracy: 82.35294117647058\n",
            "Random State: 434, Accuracy: 86.27450980392157\n",
            "Random State: 435, Accuracy: 78.43137254901961\n",
            "Random State: 436, Accuracy: 76.47058823529412\n",
            "Random State: 437, Accuracy: 88.23529411764706\n",
            "Random State: 438, Accuracy: 76.47058823529412\n",
            "Random State: 439, Accuracy: 82.35294117647058\n",
            "Random State: 440, Accuracy: 88.23529411764706\n",
            "Random State: 441, Accuracy: 82.35294117647058\n",
            "Random State: 442, Accuracy: 86.27450980392157\n",
            "Random State: 443, Accuracy: 80.3921568627451\n",
            "Random State: 444, Accuracy: 82.35294117647058\n",
            "Random State: 445, Accuracy: 82.35294117647058\n",
            "Random State: 446, Accuracy: 80.3921568627451\n",
            "Random State: 447, Accuracy: 76.47058823529412\n",
            "Random State: 448, Accuracy: 82.35294117647058\n",
            "Random State: 449, Accuracy: 90.19607843137256\n",
            "Random State: 450, Accuracy: 78.43137254901961\n",
            "Random State: 451, Accuracy: 78.43137254901961\n",
            "Random State: 452, Accuracy: 78.43137254901961\n",
            "Random State: 453, Accuracy: 82.35294117647058\n",
            "Random State: 454, Accuracy: 80.3921568627451\n",
            "Random State: 455, Accuracy: 88.23529411764706\n",
            "Random State: 456, Accuracy: 80.3921568627451\n",
            "Random State: 457, Accuracy: 84.31372549019608\n",
            "Random State: 458, Accuracy: 86.27450980392157\n",
            "Random State: 459, Accuracy: 72.54901960784314\n",
            "Random State: 460, Accuracy: 84.31372549019608\n",
            "Random State: 461, Accuracy: 80.3921568627451\n",
            "Random State: 462, Accuracy: 86.27450980392157\n",
            "Random State: 463, Accuracy: 86.27450980392157\n",
            "Random State: 464, Accuracy: 76.47058823529412\n",
            "Random State: 465, Accuracy: 78.43137254901961\n",
            "Random State: 466, Accuracy: 84.31372549019608\n",
            "Random State: 467, Accuracy: 82.35294117647058\n",
            "Random State: 468, Accuracy: 78.43137254901961\n",
            "Random State: 469, Accuracy: 78.43137254901961\n",
            "Random State: 470, Accuracy: 88.23529411764706\n",
            "Random State: 471, Accuracy: 74.50980392156863\n",
            "Random State: 472, Accuracy: 76.47058823529412\n",
            "Random State: 473, Accuracy: 86.27450980392157\n",
            "Random State: 474, Accuracy: 82.35294117647058\n",
            "Random State: 475, Accuracy: 72.54901960784314\n",
            "Random State: 476, Accuracy: 82.35294117647058\n",
            "Random State: 477, Accuracy: 80.3921568627451\n",
            "Random State: 478, Accuracy: 76.47058823529412\n",
            "Random State: 479, Accuracy: 78.43137254901961\n",
            "Random State: 480, Accuracy: 74.50980392156863\n",
            "Random State: 481, Accuracy: 88.23529411764706\n",
            "Random State: 482, Accuracy: 80.3921568627451\n",
            "Random State: 483, Accuracy: 80.3921568627451\n",
            "Random State: 484, Accuracy: 86.27450980392157\n",
            "Random State: 485, Accuracy: 80.3921568627451\n",
            "Random State: 486, Accuracy: 80.3921568627451\n",
            "Random State: 487, Accuracy: 74.50980392156863\n",
            "Random State: 488, Accuracy: 84.31372549019608\n",
            "Random State: 489, Accuracy: 86.27450980392157\n",
            "Random State: 490, Accuracy: 86.27450980392157\n",
            "Random State: 491, Accuracy: 86.27450980392157\n",
            "Random State: 492, Accuracy: 74.50980392156863\n",
            "Random State: 493, Accuracy: 82.35294117647058\n",
            "Random State: 494, Accuracy: 78.43137254901961\n",
            "Random State: 495, Accuracy: 80.3921568627451\n",
            "Random State: 496, Accuracy: 82.35294117647058\n",
            "Random State: 497, Accuracy: 88.23529411764706\n",
            "Random State: 498, Accuracy: 86.27450980392157\n",
            "Random State: 499, Accuracy: 84.31372549019608\n",
            "Random State: 500, Accuracy: 80.3921568627451\n",
            "Random State: 501, Accuracy: 74.50980392156863\n",
            "Random State: 502, Accuracy: 78.43137254901961\n",
            "Random State: 503, Accuracy: 78.43137254901961\n",
            "Random State: 504, Accuracy: 82.35294117647058\n",
            "Random State: 505, Accuracy: 90.19607843137256\n",
            "Random State: 506, Accuracy: 76.47058823529412\n",
            "Random State: 507, Accuracy: 86.27450980392157\n",
            "Random State: 508, Accuracy: 88.23529411764706\n",
            "Random State: 509, Accuracy: 84.31372549019608\n",
            "Random State: 510, Accuracy: 86.27450980392157\n",
            "Random State: 511, Accuracy: 82.35294117647058\n",
            "Random State: 512, Accuracy: 84.31372549019608\n",
            "Random State: 513, Accuracy: 84.31372549019608\n",
            "Random State: 514, Accuracy: 80.3921568627451\n",
            "Random State: 515, Accuracy: 80.3921568627451\n",
            "Random State: 516, Accuracy: 80.3921568627451\n",
            "Random State: 517, Accuracy: 76.47058823529412\n",
            "Random State: 518, Accuracy: 86.27450980392157\n",
            "Random State: 519, Accuracy: 82.35294117647058\n",
            "Random State: 520, Accuracy: 80.3921568627451\n",
            "Random State: 521, Accuracy: 76.47058823529412\n",
            "Random State: 522, Accuracy: 84.31372549019608\n",
            "Random State: 523, Accuracy: 78.43137254901961\n",
            "Random State: 524, Accuracy: 84.31372549019608\n",
            "Random State: 525, Accuracy: 78.43137254901961\n",
            "Random State: 526, Accuracy: 78.43137254901961\n",
            "Random State: 527, Accuracy: 78.43137254901961\n",
            "Random State: 528, Accuracy: 92.15686274509804\n",
            "Random State: 529, Accuracy: 90.19607843137256\n",
            "Random State: 530, Accuracy: 88.23529411764706\n",
            "Random State: 531, Accuracy: 80.3921568627451\n",
            "Random State: 532, Accuracy: 80.3921568627451\n",
            "Random State: 533, Accuracy: 88.23529411764706\n",
            "Random State: 534, Accuracy: 76.47058823529412\n",
            "Random State: 535, Accuracy: 80.3921568627451\n",
            "Random State: 536, Accuracy: 80.3921568627451\n",
            "Random State: 537, Accuracy: 78.43137254901961\n",
            "Random State: 538, Accuracy: 80.3921568627451\n",
            "Random State: 539, Accuracy: 78.43137254901961\n",
            "Random State: 540, Accuracy: 80.3921568627451\n",
            "Random State: 541, Accuracy: 80.3921568627451\n",
            "Random State: 542, Accuracy: 80.3921568627451\n",
            "Random State: 543, Accuracy: 80.3921568627451\n",
            "Random State: 544, Accuracy: 78.43137254901961\n",
            "Random State: 545, Accuracy: 74.50980392156863\n",
            "Random State: 546, Accuracy: 90.19607843137256\n",
            "Random State: 547, Accuracy: 86.27450980392157\n",
            "Random State: 548, Accuracy: 86.27450980392157\n",
            "Random State: 549, Accuracy: 84.31372549019608\n",
            "Random State: 550, Accuracy: 82.35294117647058\n",
            "Random State: 551, Accuracy: 78.43137254901961\n",
            "Random State: 552, Accuracy: 82.35294117647058\n",
            "Random State: 553, Accuracy: 82.35294117647058\n",
            "Random State: 554, Accuracy: 80.3921568627451\n",
            "Random State: 555, Accuracy: 84.31372549019608\n",
            "Random State: 556, Accuracy: 82.35294117647058\n",
            "Random State: 557, Accuracy: 80.3921568627451\n",
            "Random State: 558, Accuracy: 82.35294117647058\n",
            "Random State: 559, Accuracy: 84.31372549019608\n",
            "Random State: 560, Accuracy: 82.35294117647058\n",
            "Random State: 561, Accuracy: 86.27450980392157\n",
            "Random State: 562, Accuracy: 90.19607843137256\n",
            "Random State: 563, Accuracy: 80.3921568627451\n",
            "Random State: 564, Accuracy: 80.3921568627451\n",
            "Random State: 565, Accuracy: 90.19607843137256\n",
            "Random State: 566, Accuracy: 80.3921568627451\n",
            "Random State: 567, Accuracy: 78.43137254901961\n",
            "Random State: 568, Accuracy: 90.19607843137256\n",
            "Random State: 569, Accuracy: 74.50980392156863\n",
            "Random State: 570, Accuracy: 78.43137254901961\n",
            "Random State: 571, Accuracy: 86.27450980392157\n",
            "Random State: 572, Accuracy: 86.27450980392157\n",
            "Random State: 573, Accuracy: 84.31372549019608\n",
            "Random State: 574, Accuracy: 78.43137254901961\n",
            "Random State: 575, Accuracy: 92.15686274509804\n",
            "Random State: 576, Accuracy: 78.43137254901961\n",
            "Random State: 577, Accuracy: 78.43137254901961\n",
            "Random State: 578, Accuracy: 82.35294117647058\n",
            "Random State: 579, Accuracy: 86.27450980392157\n",
            "Random State: 580, Accuracy: 76.47058823529412\n",
            "Random State: 581, Accuracy: 82.35294117647058\n",
            "Random State: 582, Accuracy: 82.35294117647058\n",
            "Random State: 583, Accuracy: 78.43137254901961\n",
            "Random State: 584, Accuracy: 84.31372549019608\n",
            "Random State: 585, Accuracy: 76.47058823529412\n",
            "Random State: 586, Accuracy: 82.35294117647058\n",
            "Random State: 587, Accuracy: 84.31372549019608\n",
            "Random State: 588, Accuracy: 80.3921568627451\n",
            "Random State: 589, Accuracy: 84.31372549019608\n",
            "Random State: 590, Accuracy: 84.31372549019608\n",
            "Random State: 591, Accuracy: 70.58823529411765\n",
            "Random State: 592, Accuracy: 84.31372549019608\n",
            "Random State: 593, Accuracy: 84.31372549019608\n",
            "Random State: 594, Accuracy: 80.3921568627451\n",
            "Random State: 595, Accuracy: 92.15686274509804\n",
            "Random State: 596, Accuracy: 78.43137254901961\n",
            "Random State: 597, Accuracy: 84.31372549019608\n",
            "Random State: 598, Accuracy: 84.31372549019608\n",
            "Random State: 599, Accuracy: 76.47058823529412\n",
            "Random State: 600, Accuracy: 80.3921568627451\n",
            "Random State: 601, Accuracy: 84.31372549019608\n",
            "Random State: 602, Accuracy: 76.47058823529412\n",
            "Random State: 603, Accuracy: 74.50980392156863\n",
            "Random State: 604, Accuracy: 86.27450980392157\n",
            "Random State: 605, Accuracy: 86.27450980392157\n",
            "Random State: 606, Accuracy: 84.31372549019608\n",
            "Random State: 607, Accuracy: 84.31372549019608\n",
            "Random State: 608, Accuracy: 82.35294117647058\n",
            "Random State: 609, Accuracy: 86.27450980392157\n",
            "Random State: 610, Accuracy: 74.50980392156863\n",
            "Random State: 611, Accuracy: 80.3921568627451\n",
            "Random State: 612, Accuracy: 86.27450980392157\n",
            "Random State: 613, Accuracy: 88.23529411764706\n",
            "Random State: 614, Accuracy: 76.47058823529412\n",
            "Random State: 615, Accuracy: 80.3921568627451\n",
            "Random State: 616, Accuracy: 80.3921568627451\n",
            "Random State: 617, Accuracy: 82.35294117647058\n",
            "Random State: 618, Accuracy: 82.35294117647058\n",
            "Random State: 619, Accuracy: 90.19607843137256\n",
            "Random State: 620, Accuracy: 82.35294117647058\n",
            "Random State: 621, Accuracy: 78.43137254901961\n",
            "Random State: 622, Accuracy: 78.43137254901961\n",
            "Random State: 623, Accuracy: 80.3921568627451\n",
            "Random State: 624, Accuracy: 76.47058823529412\n",
            "Random State: 625, Accuracy: 84.31372549019608\n",
            "Random State: 626, Accuracy: 74.50980392156863\n",
            "Random State: 627, Accuracy: 78.43137254901961\n",
            "Random State: 628, Accuracy: 78.43137254901961\n",
            "Random State: 629, Accuracy: 76.47058823529412\n",
            "Random State: 630, Accuracy: 88.23529411764706\n",
            "Random State: 631, Accuracy: 86.27450980392157\n",
            "Random State: 632, Accuracy: 82.35294117647058\n",
            "Random State: 633, Accuracy: 84.31372549019608\n",
            "Random State: 634, Accuracy: 74.50980392156863\n",
            "Random State: 635, Accuracy: 78.43137254901961\n",
            "Random State: 636, Accuracy: 86.27450980392157\n",
            "Random State: 637, Accuracy: 84.31372549019608\n",
            "Random State: 638, Accuracy: 80.3921568627451\n",
            "Random State: 639, Accuracy: 82.35294117647058\n",
            "Random State: 640, Accuracy: 78.43137254901961\n",
            "Random State: 641, Accuracy: 80.3921568627451\n",
            "Random State: 642, Accuracy: 78.43137254901961\n",
            "Random State: 643, Accuracy: 80.3921568627451\n",
            "Random State: 644, Accuracy: 88.23529411764706\n",
            "Random State: 645, Accuracy: 86.27450980392157\n",
            "Random State: 646, Accuracy: 80.3921568627451\n",
            "Random State: 647, Accuracy: 82.35294117647058\n",
            "Random State: 648, Accuracy: 72.54901960784314\n",
            "Random State: 649, Accuracy: 92.15686274509804\n",
            "Random State: 650, Accuracy: 82.35294117647058\n",
            "Random State: 651, Accuracy: 82.35294117647058\n",
            "Random State: 652, Accuracy: 78.43137254901961\n",
            "Random State: 653, Accuracy: 84.31372549019608\n",
            "Random State: 654, Accuracy: 88.23529411764706\n",
            "Random State: 655, Accuracy: 72.54901960784314\n",
            "Random State: 656, Accuracy: 84.31372549019608\n",
            "Random State: 657, Accuracy: 86.27450980392157\n",
            "Random State: 658, Accuracy: 84.31372549019608\n",
            "Random State: 659, Accuracy: 80.3921568627451\n",
            "Random State: 660, Accuracy: 74.50980392156863\n",
            "Random State: 661, Accuracy: 88.23529411764706\n",
            "Random State: 662, Accuracy: 74.50980392156863\n",
            "Random State: 663, Accuracy: 84.31372549019608\n",
            "Random State: 664, Accuracy: 90.19607843137256\n",
            "Random State: 665, Accuracy: 84.31372549019608\n",
            "Random State: 666, Accuracy: 84.31372549019608\n",
            "Random State: 667, Accuracy: 78.43137254901961\n",
            "Random State: 668, Accuracy: 86.27450980392157\n",
            "Random State: 669, Accuracy: 76.47058823529412\n",
            "Random State: 670, Accuracy: 80.3921568627451\n",
            "Random State: 671, Accuracy: 84.31372549019608\n",
            "Random State: 672, Accuracy: 82.35294117647058\n",
            "Random State: 673, Accuracy: 86.27450980392157\n",
            "Random State: 674, Accuracy: 82.35294117647058\n",
            "Random State: 675, Accuracy: 86.27450980392157\n",
            "Random State: 676, Accuracy: 78.43137254901961\n",
            "Random State: 677, Accuracy: 88.23529411764706\n",
            "Random State: 678, Accuracy: 82.35294117647058\n",
            "Random State: 679, Accuracy: 82.35294117647058\n",
            "Random State: 680, Accuracy: 82.35294117647058\n",
            "Random State: 681, Accuracy: 86.27450980392157\n",
            "Random State: 682, Accuracy: 92.15686274509804\n",
            "Random State: 683, Accuracy: 78.43137254901961\n",
            "Random State: 684, Accuracy: 82.35294117647058\n",
            "Random State: 685, Accuracy: 82.35294117647058\n",
            "Random State: 686, Accuracy: 84.31372549019608\n",
            "Random State: 687, Accuracy: 68.62745098039215\n",
            "Random State: 688, Accuracy: 70.58823529411765\n",
            "Random State: 689, Accuracy: 82.35294117647058\n",
            "Random State: 690, Accuracy: 88.23529411764706\n",
            "Random State: 691, Accuracy: 86.27450980392157\n",
            "Random State: 692, Accuracy: 86.27450980392157\n",
            "Random State: 693, Accuracy: 74.50980392156863\n",
            "Random State: 694, Accuracy: 80.3921568627451\n",
            "Random State: 695, Accuracy: 88.23529411764706\n",
            "Random State: 696, Accuracy: 84.31372549019608\n",
            "Random State: 697, Accuracy: 84.31372549019608\n",
            "Random State: 698, Accuracy: 78.43137254901961\n",
            "Random State: 699, Accuracy: 80.3921568627451\n",
            "Random State: 700, Accuracy: 86.27450980392157\n",
            "Random State: 701, Accuracy: 78.43137254901961\n",
            "Random State: 702, Accuracy: 76.47058823529412\n",
            "Random State: 703, Accuracy: 76.47058823529412\n",
            "Random State: 704, Accuracy: 86.27450980392157\n",
            "Random State: 705, Accuracy: 72.54901960784314\n",
            "Random State: 706, Accuracy: 78.43137254901961\n",
            "Random State: 707, Accuracy: 80.3921568627451\n",
            "Random State: 708, Accuracy: 86.27450980392157\n",
            "Random State: 709, Accuracy: 94.11764705882352\n",
            "Akurasi mencapai target (0.93) dengan random state 709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8m99FECi180",
        "outputId": "477c9fd9-1337-4144-9e1b-662f302c43e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9411764705882353\n",
            "\n",
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "      corpus alineum       1.00      0.80      0.89         5\n",
            "           hordeolum       1.00      0.93      0.96        14\n",
            "      katarak imatur       1.00      1.00      1.00         8\n",
            "      konjungtivitis       0.80      0.80      0.80         5\n",
            "          presbiopia       1.00      1.00      1.00         9\n",
            "syndroma mata kering       0.83      1.00      0.91        10\n",
            "\n",
            "            accuracy                           0.94        51\n",
            "           macro avg       0.94      0.92      0.93        51\n",
            "        weighted avg       0.95      0.94      0.94        51\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=plt.cm.Blues, square=True)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title(\"Confusion Matrix (with best parameters)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "81QdUUOdtz-N",
        "outputId": "c57337f0-46b2-4554-af64-44131500a95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Nyoba Random Forest**"
      ],
      "metadata": {
        "id": "ZsIsDZ1uvDvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "QcO9UgOlvClz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_random_state = 200  # Ganti dengan nilai maksimum yang sesuai\n",
        "target_accuracy = 0.93\n",
        "best_accuracy = 0.0\n",
        "best_random_state = 0"
      ],
      "metadata": {
        "id": "WuzxMKQLxAUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, max_random_state + 1):\n",
        "  x = df_data.drop(columns = 'Diagnosa', axis = 1)\n",
        "  y = df_data['Diagnosa']\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=i)\n",
        "  model = RandomForestClassifier(n_estimators=300, random_state=i)  # Ganti parameter sesuai kebutuhan Anda\n",
        "  model.fit(x_train, y_train)\n",
        "  y_pred = model.predict(x_test)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "  if accuracy >= target_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_random_state = i\n",
        "        print(f\"Akurasi mencapai target ({target_accuracy}) dengan random state {i}\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99VO1ST9wKRK",
        "outputId": "7440ac59-7194-43ef-fa90-f7129f1db35e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.868421052631579\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.7631578947368421\n",
            "Accuracy: 0.8421052631578947\n",
            "Accuracy: 0.868421052631579\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.7368421052631579\n",
            "Accuracy: 0.8421052631578947\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.9210526315789473\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.7105263157894737\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.75\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.7105263157894737\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.7105263157894737\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.868421052631579\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.8421052631578947\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.7631578947368421\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.6973684210526315\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.8421052631578947\n",
            "Accuracy: 0.7631578947368421\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.8421052631578947\n",
            "Accuracy: 0.7631578947368421\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.7368421052631579\n",
            "Accuracy: 0.75\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.8421052631578947\n",
            "Accuracy: 0.7368421052631579\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.868421052631579\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.7631578947368421\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.7631578947368421\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.75\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.75\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.8421052631578947\n",
            "Accuracy: 0.881578947368421\n",
            "Accuracy: 0.7631578947368421\n",
            "Accuracy: 0.75\n",
            "Accuracy: 0.7368421052631579\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.8421052631578947\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.8421052631578947\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.8421052631578947\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.75\n",
            "Accuracy: 0.7631578947368421\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.7105263157894737\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.8421052631578947\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.75\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.7631578947368421\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.8947368421052632\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.8947368421052632\n",
            "Accuracy: 0.7631578947368421\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.868421052631579\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.7631578947368421\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.8421052631578947\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.7236842105263158\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.868421052631579\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.8421052631578947\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.868421052631579\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.868421052631579\n",
            "Accuracy: 0.7631578947368421\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.75\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.8421052631578947\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.8947368421052632\n",
            "Accuracy: 0.75\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.7631578947368421\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.881578947368421\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.868421052631579\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.7631578947368421\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.8421052631578947\n",
            "Accuracy: 0.8289473684210527\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.7763157894736842\n",
            "Accuracy: 0.8157894736842105\n",
            "Accuracy: 0.7368421052631579\n",
            "Accuracy: 0.8552631578947368\n",
            "Accuracy: 0.8026315789473685\n",
            "Accuracy: 0.7894736842105263\n",
            "Accuracy: 0.8157894736842105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nClassification Report:\\n', classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAP16FpXwy9G",
        "outputId": "9d6f40a3-80e9-40e7-ccb5-cd8378966a24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "      corpus alineum       1.00      0.67      0.80         3\n",
            "           hordeolum       0.93      1.00      0.97        14\n",
            "      katarak imatur       1.00      0.82      0.90        11\n",
            "      konjungtivitis       1.00      1.00      1.00         3\n",
            "          presbiopia       1.00      0.93      0.97        15\n",
            "syndroma mata kering       0.62      1.00      0.77         5\n",
            "\n",
            "            accuracy                           0.92        51\n",
            "           macro avg       0.93      0.90      0.90        51\n",
            "        weighted avg       0.94      0.92      0.92        51\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=plt.cm.Blues, square=True)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title(\"Confusion Matrix (with best parameters)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "O2pz9Fjwz9to",
        "outputId": "f0debfca-bfe0-4959-ae11-1359ddb14728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
